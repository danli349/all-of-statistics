{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression** is a method for studying the relationship between a **response variable** $Y$ and a **covariates** $X$.  The covariate is also called a **predictor variable** or **feature**.  Later we will generalize and allow for more than one covariate.  The data are of the form\n",
    "\n",
    "$$ (Y_1, X_1), \\dots, (Y_n, X_n) $$\n",
    "\n",
    "One way to summarize the relationship between $X$ and $Y$ is through the **regression function**\n",
    "\n",
    "$$ r(x) = \\mathbb{E}(Y | X = x) = \\int y f(y | x) dy $$\n",
    "\n",
    "Most of this chapter is concerned with estimating the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest version of regression is when $X_i$ is simple (a scalar, not a vector) and $r(x)$ is assumed to be linear:\n",
    "\n",
    "$$r(x) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "This model is called the **simple linear regression model**.  Let $\\epsilon_i = Y_i - (\\beta_0 + \\beta_1 X_i)$.  Then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(\\epsilon_i | Y_i) &= \\mathbb{E}(Y_i - (\\beta_0 + \\beta_1 X_i) | X_i)\\\\\n",
    "&= \\mathbb{E}(Y_i | X_i) - (\\beta_0 + \\beta_1 X_i)\\\\\n",
    "&= r(X_i) - (\\beta_0 + \\beta_1 X_i)\\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let $\\sigma^2(x) = \\mathbb{V}(\\epsilon_i | X_i = x)$.  We will make the further simplifying assumption that $\\sigma^2(x) = \\sigma^2$ does not depend on $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Linear Regression Model**\n",
    "\n",
    "$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\n",
    "\n",
    "where $\\mathbb{E}(\\epsilon_i | X_i) = 0$ and $\\mathbb{V}(\\epsilon_i | X_i) = \\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unknown models in the parameter are the intercept $\\beta_0$, the slope $\\beta_1$ and the variance $\\sigma^2$.  Let $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ denote the estimates of $\\beta_0$ and $\\beta_1$.  The **fitted line** is defined to be\n",
    "\n",
    "$$\\hat{r}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **predicted values** or **fitted values** are $\\hat{Y}_i = \\hat{r}(X_i)$ and the **residuals** are defined to be\n",
    "\n",
    "$$\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **residual sum of squares** or RSS is defined by\n",
    "\n",
    "$$ \\text{RSS} = \\sum_{i=1}^n \\hat{\\epsilon}_i^2$$\n",
    "\n",
    "The quantity RSS measures how well the fitted line fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **least squares estimates** are the values $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize $\\text{RSS} = \\sum_{i=1}^n \\hat{\\epsilon}_i^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.4**.  The least square estimates are given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (X_i - \\overline{X}_n) (Y_i - \\overline{Y}_n)}{\\sum_{i=1}^n (X_i - \\overline{X}_n)^2}\\\\\n",
    "\\hat{\\beta}_0 &= \\overline{Y}_n - \\hat{\\beta}_1 \\overline{X}_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "An unbiased estimate of $\\sigma^2$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\left( \\frac{1}{n - 2} \\right) \\sum_{i=1}^n \\hat{\\epsilon}_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Least Squares and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we add the assumption that $\\epsilon_i | X_i \\sim N(0, \\sigma^2)$, that is,\n",
    "\n",
    "$Y_i | X_i \\sim N(\\mu_i, \\sigma_i^2)$\n",
    "\n",
    "where $\\mu_i = \\beta_0 + \\beta_i X_i$.  The likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_{i=1}n f(X_i, Y_i) &= \\prod_{i=1}^n f_X(X_i) f_{Y|X}(Y_i | X_i)\\\\\n",
    "&= \\prod_{i=1}^n f_X(X_i) \\times \\prod_{i=1}^n f_{Y|X}(Y_i | X_i) \\\\\n",
    "&= \\mathcal{L}_1 \\times \\mathcal{L}_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}_1 = \\prod_{i=1}^n f_X(X_i)$ and $\\mathcal{L}_2 = \\prod_{i=1}^n f_{Y|X}(Y_i | X_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $\\mathcal{L}_1$ does not involve the parameters $\\beta_0$ and $\\beta_1$.  We shall focus on the second term $\\mathcal{L}_2$ which is called the **conditional likelihood**, given by\n",
    "\n",
    "$$\\mathcal{L}_2 \\equiv \\mathcal{L}(\\beta_0, \\beta_1, \\sigma)\n",
    "= \\prod_{i=1}^n f_{Y|X}(Y_i | X_i)\n",
    "\\propto \\sigma^{-n} \\exp \\left\\{ - \\frac{1}{2 \\sigma^2} \\sum_i (Y_i - \\mu_i)^2 \\right\\}\n",
    "$$\n",
    "\n",
    "The conditional log-likelihood is\n",
    "\n",
    "$$\\ell(\\beta_0, \\beta_1, \\sigma) = -n \\log \\sigma - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n \\left(Y_i - (\\beta_0 + \\beta_1 X_i) \\right)^2$$\n",
    "\n",
    "To find the MLE of $(\\beta_0, \\beta_1)$ we maximize the conditional log likelihood. We can see from the equation above that this is the same as minimizing the RSS.  Therefore, we have shown the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.7**.  Under the assumption of Normality, the least squares estimator is also the maximum likelihood estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also maximize $\\ell(\\beta_0, \\beta_1, \\sigma)$ over $\\sigma$ yielding the MLE\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\epsilon}_i^2 $$\n",
    "\n",
    "This estimator is similar to, but not identical to, the unbiased estimator.  Common practice is to use the unbiased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 Properties of the Least Squares Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.8**.  Let $\\hat{\\beta}^T = (\\hat{\\beta}_0, \\hat{\\beta}_1)^T$ denote the least squares estimators.  Then,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\beta} | X^n) = \\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\hat{\\beta} | X^n) = \\frac{\\sigma^2}{n s_X^2} \\begin{pmatrix} \n",
    "\\frac{1}{n} \\sum_{i=1}^n X_i^2 & -\\overline{X}_n \\\\\n",
    "-\\overline{X}_n & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $s_X^2 = n^{-1} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated standard errors of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are obtained by taking the square roots of the corresponding diagonal terms of $\\mathbb{V}(\\hat{\\beta} | X^n)$ and inserting the estimate $\\hat{\\sigma}$ for $\\sigma$.  Thus,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_0) &= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\sqrt{\\frac{\\sum_{i=1}^n X_i^2}{n}}\\\\\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_1) &= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We should write $\\hat{\\text{se}}(\\hat{\\beta}_0 | X^n)$ and $\\hat{\\text{se}}(\\hat{\\beta}_1 | X^n)$ but we will use the shorter notation $\\hat{\\text{se}}(\\hat{\\beta}_0)$ and $\\hat{\\text{se}}(\\hat{\\beta}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* univariate linear regression models:  \n",
    "When there are $n$ response variables $Y_i, i=1,2,\\cdots,n$ and $r$ predictor variables $Z_{ij},j=1,2,\\cdots,r$ for each response variable:\n",
    "$$\\begin{bmatrix}\n",
    "Y_1\\\\\n",
    "Y_2\\\\\n",
    "\\vdots\\\\\n",
    "Y_n\\\\\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "1&Z_{11}&Z_{12}&\\cdots&Z_{1r}\\\\\n",
    "1&Z_{21}&Z_{22}&\\cdots&Z_{2r}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "1&Z_{n1}&Z_{n2}&\\cdots&Z_{nr}\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_r\\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "\\epsilon_1\\\\\n",
    "\\epsilon_2\\\\\n",
    "\\vdots\\\\\n",
    "\\epsilon_n\\\\\n",
    "\\end{bmatrix}$$  or\n",
    "$\\mathbf Y=\\mathbf Z\\boldsymbol\\beta+\\boldsymbol\\epsilon$, where $E(\\boldsymbol\\epsilon)=\\boldsymbol0$, $Cov(\\boldsymbol\\epsilon)=E(\\boldsymbol\\epsilon\\boldsymbol\\epsilon^T)=\\sigma^2\\mathbf I$. We have to find the regression coefficients $\\boldsymbol\\beta$ and the error variance $\\sigma^2$ that are consistent with the available data. Denote $\\hat{\\boldsymbol\\beta}$ as least squares estimate of $\\boldsymbol\\beta$, then $\\hat{\\boldsymbol y}=\\mathbf Z\\hat{\\boldsymbol\\beta}$ and $\\hat{\\boldsymbol y}$ is the projection of vector $\\boldsymbol y$ on the column space of $\\mathbf Z$, so $\\boldsymbol y-\\hat{\\boldsymbol y}=\\boldsymbol y-\\mathbf Z\\hat{\\boldsymbol\\beta}$ is perpendicular to column space of $\\mathbf Z$, so $\\mathbf Z^T(\\boldsymbol y-\\mathbf Z\\hat{\\boldsymbol\\beta})=\\mathbf Z^T\\boldsymbol y-\\mathbf Z^T\\mathbf Z\\hat{\\boldsymbol\\beta}=\\boldsymbol0\\Rightarrow\\mathbf Z^T\\boldsymbol y=\\mathbf Z^T\\mathbf Z\\hat{\\boldsymbol\\beta}$. $\\mathbf Z^T\\mathbf Z$ is a $(r+1)(r+1)$ asymmetric matrix. Because the column space of $\\mathbf Z^T\\mathbf Z$ must in the column space of $\\mathbf Z$ and row space of $\\mathbf Z^T\\mathbf Z$ must in the row space of $\\mathbf Z$ so the rank of $rank(\\mathbf Z^T\\mathbf Z)\\le r+1$ and $rank(\\mathbf Z^T\\mathbf Z)\\le n$, because $\\mathbf Z^T\\mathbf Z$ is a $(r+1)(r+1)$ matrix, so only when $r+1\\le n$, it will be possible that $rank(\\mathbf Z^T\\mathbf Z)=r+1$ and $\\mathbf Z^T\\mathbf Z$ is invertible.   \n",
    "Then $\\hat{\\boldsymbol\\beta}=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol y$ and $\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol y=\\hat{\\boldsymbol y}$  , so $\\boldsymbol y-\\hat{\\boldsymbol y}=(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol y$ It is clear that every column vector of $(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)$ is perpendicular to column space of $\\mathbf Z$ caused by $\\mathbf Z^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)=\\mathbf0$.  And $(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)=\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T$, so $\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T$ is an <span style=\"color: red;\">**Idempotent matrix**</span>. So $$\\begin{align}\n",
    "tr[\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T]&=tr[\\mathbf 1]-tr[\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T]\\\\\n",
    "&=tr[\\mathbf 1]-tr[\\mathbf Z^T\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}]\\\\\n",
    "&=tr[\\underset{n\\times n}{\\mathbf 1}]-tr[\\underset{(r+1)\\times (r+1)}{\\mathbf 1}]\\\\\n",
    "&=n-r-1\n",
    "\\end{align}$$    \n",
    "Because $\\hat{\\boldsymbol\\epsilon}=\\boldsymbol y-\\hat{\\boldsymbol y}$ is perpendicular to column space of $\\mathbf Z$, then $\\mathbf Z^T\\hat{\\boldsymbol\\epsilon}=\\boldsymbol0$ and $\\boldsymbol1^T\\hat{\\boldsymbol\\epsilon}=\\displaystyle\\sum_{i=1}^{n}\\hat{\\boldsymbol\\epsilon}_i=\\displaystyle\\sum_{i=1}^{n}y_i-\\displaystyle\\sum_{i=1}^{n}\\hat y_i=n\\bar y-n\\bar{\\hat y}=0$. Then $\\bar y=\\bar{\\hat y}$ And because $\\mathbf y^T\\mathbf y=(\\hat{\\mathbf y}+\\mathbf y-\\hat{\\mathbf y})^T(\\hat{\\mathbf y}+\\mathbf y-\\hat{\\mathbf y})=(\\hat{\\mathbf y}+\\hat{\\boldsymbol\\epsilon})^T(\\hat{\\mathbf y}+\\hat{\\boldsymbol\\epsilon})=\\hat{\\mathbf y}^T\\hat{\\mathbf y}+\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}$, this also can be get using Pythagorean theorem. Then sum of squares decomposition can be: $\\mathbf y^T\\mathbf y-n(\\bar y)^2=\\hat{\\mathbf y}^T\\hat{\\mathbf y}-n(\\bar{\\hat y})^2+\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}$ or $\\underset{SS_{total}}{\\displaystyle\\sum_{i=1}^{n}(y_i-\\bar y)^2}=\\underset{SS_{regression}}{\\displaystyle\\sum_{i=1}^{n}(\\hat y_i-\\bar{\\hat y})^2}+\\underset{SS_{error}}{\\displaystyle\\sum_{i=1}^{n}\\hat\\epsilon_i^2}$  The quality of the models fit can be measured by the **coefficient of determination** $R^2=\\frac{\\displaystyle\\sum_{i=1}^{n}(\\hat y_i-\\bar{\\hat y})^2}{\\displaystyle\\sum_{i=1}^{n}(y_i-\\bar y)^2}$\n",
    "\n",
    "* Inferences About the Regression Model:  \n",
    "1) Because $E(\\hat{\\boldsymbol\\beta})=E((\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\mathbf Y)$, so $E(\\mathbf Z\\hat{\\boldsymbol\\beta})=E(\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\mathbf Y)=E(\\hat{\\mathbf Y})$ and because  \n",
    "$E(\\mathbf Z\\boldsymbol\\beta)=E(\\mathbf Y-\\boldsymbol\\epsilon)=E(\\mathbf Y)=E(\\hat{\\mathbf Y})$, so $E(\\mathbf Z\\hat{\\boldsymbol\\beta})=E(\\mathbf Z\\boldsymbol\\beta)$ and $E(\\mathbf Z(\\hat{\\boldsymbol\\beta}-\\boldsymbol\\beta))=\\boldsymbol0$ so $E(\\hat{\\boldsymbol\\beta})=E(\\boldsymbol\\beta)$   \n",
    "2) If $\\boldsymbol\\epsilon$ is distributed as $N_n(\\boldsymbol0,\\sigma^2\\mathbf I)$, then $Cov(\\mathbf Z\\hat{\\boldsymbol\\beta})=\\mathbf Z^T\\mathbf ZCov(\\hat{\\boldsymbol\\beta})=Cov(\\hat{\\mathbf Y})=Cov(\\mathbf Y+\\boldsymbol\\epsilon)=Cov(\\boldsymbol\\epsilon)=\\sigma^2\\mathbf I$, then $Cov(\\hat{\\boldsymbol\\beta})=\\sigma^2(\\mathbf Z^T\\mathbf Z)^{-1}$, then $\\hat{\\boldsymbol\\beta}$ is distributed as $N_{(r+1)}(\\boldsymbol\\beta,\\sigma^2(\\mathbf Z^T\\mathbf Z)^{-1})$  \n",
    "Because $E(\\hat{\\boldsymbol\\epsilon})=\\boldsymbol 0$ and $Cov(\\hat{\\boldsymbol\\epsilon})=Cov(\\boldsymbol y-\\hat{\\boldsymbol y})=Cov((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol y)=(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\sigma^2$ $$\\begin{align}\n",
    "E(\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon})&=E(\\boldsymbol y-\\hat{\\boldsymbol y})^T(\\boldsymbol y-\\hat{\\boldsymbol y})\\\\\n",
    "&=E((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol y)^T((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol y)\\\\\n",
    "&=E(\\boldsymbol y^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T))((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol y)\\\\\n",
    "&=E(\\boldsymbol y^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol y)\\\\\n",
    "&=E(\\boldsymbol y^T\\boldsymbol y)-E(\\boldsymbol y^T\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol y)\\\\\n",
    "&=n\\sigma^2-(r+1)\\sigma^2\\\\\n",
    "&=(n-r-1)\\sigma^2\\\\\n",
    "\\end{align}$$ so defining $$s^2=\\frac{\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}}{n-r-1}=\\frac{\\boldsymbol y^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol y}{n-r-1}$$ Then $E(s^2)=\\sigma^2$\n",
    "3) If $\\hat\\sigma^2$ is the maximum likelihood estimator of $\\sigma^2$, then $n\\hat\\sigma^2=(n-r-1)s^2=\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}$ is distributed as $\\sigma^2\\chi_{n-r-1}^2$.  The likelihood associated with the parameters $\\boldsymbol\\beta$ and $\\sigma^2$ is $L(\\boldsymbol\\beta,\\sigma^2)=\\frac{1}{(2\\pi)^{n/2}\\sigma^n}exp\\Bigl[-\\frac{(\\mathbf y-\\mathbf Z\\boldsymbol\\beta)^T(\\mathbf y-\\mathbf Z\\boldsymbol\\beta)}{2\\sigma^2}\\Bigr]$ with the maximum occurring at $\\hat{\\boldsymbol\\beta}=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol y$. For the maximum of $\\hat\\sigma^2=(\\mathbf y-\\mathbf Z\\hat{\\boldsymbol\\beta})^T(\\mathbf y-\\mathbf Z\\hat{\\boldsymbol\\beta})/n$, then the maximum likelihood is $L(\\hat{\\boldsymbol\\beta},\\hat\\sigma^2)=\\displaystyle\\frac{1}{(2\\pi)^{n/2}\\hat\\sigma^n}e^{-n/2}$  \n",
    "4) Let $\\mathbf V=(\\mathbf Z^T\\mathbf Z)^{1/2}(\\hat{\\boldsymbol\\beta}-\\boldsymbol\\beta)$, then $E(\\mathbf V)=\\mathbf0$ and $Cov(\\mathbf V)=(\\mathbf Z^T\\mathbf Z)^{1/2}Cov(\\hat{\\boldsymbol\\beta}-\\boldsymbol\\beta)(\\mathbf Z^T\\mathbf Z)^{1/2}=(\\mathbf Z^T\\mathbf Z)^{1/2}Cov(\\hat{\\boldsymbol\\beta})(\\mathbf Z^T\\mathbf Z)^{1/2}=(\\mathbf Z^T\\mathbf Z)^{1/2}\\sigma^2(\\mathbf Z^T\\mathbf Z)^{-1}(\\mathbf Z^T\\mathbf Z)^{1/2}=\\sigma^2\\mathbf I$, so $\\mathbf V$ is normally distributed and $\\mathbf V^T\\mathbf V=(\\hat{\\boldsymbol\\beta}-\\boldsymbol\\beta)^T(\\mathbf Z^T\\mathbf Z)^{1/2}(\\mathbf Z^T\\mathbf Z)^{1/2}(\\hat{\\boldsymbol\\beta}-\\boldsymbol\\beta)=(\\hat{\\boldsymbol\\beta}-\\boldsymbol\\beta)^T(\\mathbf Z^T\\mathbf Z)(\\hat{\\boldsymbol\\beta}-\\boldsymbol\\beta)$ is distributed as $\\sigma^2\\chi_{r+1}^2$, then $\\frac{\\chi_{r+1}^2/(r+1)}{\\chi_{n-r-1}^2/(n-r-1)}=\\frac{\\mathbf V^T\\mathbf V}{(r+1)s^2}$ has an $F_{(r+1),(n-r-1)}$ distribution.  Then a $100(1-\\alpha)\\%$ confidence region for $\\boldsymbol\\beta$ is given by: $(\\boldsymbol\\beta-\\hat{\\boldsymbol\\beta})^T(\\mathbf Z^T\\mathbf Z)(\\boldsymbol\\beta-\\hat{\\boldsymbol\\beta})\\le(r+1)s^2F_{(r+1),(n-r-1)}(\\alpha)$. Also for each component of $\\boldsymbol\\beta$, $|\\beta_i-\\hat{\\beta}_i|\\le\\sqrt{(r+1)F_{(r+1),(n-r-1)}(\\alpha)}\\sqrt{s^2(\\mathbf Z^T\\mathbf Z)_{ii}^{-1}}$, where $(\\mathbf Z^T\\mathbf Z)_{ii}^{-1}$ is the $i^{th}$ diagonal element of $(\\mathbf Z^T\\mathbf Z)^{-1}$. Then, the simultaneous $100(1-\\alpha)\\%$ confidence intervals for the $\\beta_i$ are given by: $\\hat{\\beta}_i\\pm\\sqrt{(r+1)F_{(r+1),(n-r-1)}(\\alpha)}\\sqrt{s^2(\\mathbf Z^T\\mathbf Z)_{ii}^{-1}}$  \n",
    "5) If some of $\\mathbf Z=[z_0,z_1,z_2,\\cdots,z_q,z_{q+1},z_{q+2},\\cdots,z_{r}]$ parameters $[z_{q+1},z_{q+2},\\cdots,z_{r}]$ do not influence $\\mathbf y$, which means the hypothesis $H_0:\\beta_{q+1}=\\beta_{q+2}=\\cdots=\\beta_{r}=0$. We can express the general linear model as $$\\mathbf y=\\mathbf Z\\boldsymbol\\beta+\\boldsymbol\\epsilon=\\left[\\begin{array}{c:c}\n",
    "\\underset{n\\times(q+1)}{\\mathbf Z_1}&\\underset{n\\times(r-q)}{\\mathbf Z_2}\n",
    "\\end{array}\n",
    "\\right]\\begin{bmatrix}\n",
    "\\underset{(q+1)\\times1}{\\boldsymbol \\beta_{(1)}}\\\\\n",
    "\\hdashline\n",
    "\\underset{(r-q)\\times1}{\\boldsymbol \\beta_{(2)}}\\\\\n",
    "\\end{bmatrix}+\\boldsymbol\\epsilon=\\mathbf Z_1\\boldsymbol\\beta_{(1)}+\\mathbf Z_2\\boldsymbol\\beta_{(2)}+\\boldsymbol\\epsilon$$ Now the hypothesis is $H_0:\\boldsymbol\\beta_{(2)}=\\boldsymbol0,\\mathbf y=\\mathbf Z_1\\boldsymbol\\beta_{(1)}+\\boldsymbol\\epsilon$ The $$\\begin{align}\n",
    "\\text{Extra sum of squares}&=SS_{\\text{error}}(\\mathbf Z_1)-SS_{\\text{error}}(\\mathbf Z)\\\\\n",
    "&=(\\mathbf y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})^T(\\mathbf y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})-(\\mathbf y-\\mathbf Z\\hat{\\boldsymbol\\beta})^T(\\mathbf y-\\mathbf Z\\hat{\\boldsymbol\\beta})\n",
    "\\end{align}$$ where $\\hat{\\boldsymbol\\beta}_{(1)}=(\\mathbf Z_1^T\\mathbf Z_1)^{-1}\\mathbf Z_1^T\\mathbf y$. Under the restriction of the null hypothesis, the maximum likelihood is $\\underset{\\boldsymbol\\beta_{(1)},\\sigma^2}{\\text{max }}L(\\boldsymbol\\beta_{(1)},\\sigma^2)=\\displaystyle\\frac{1}{(2\\pi)^{n/2}\\hat\\sigma_1^n}e^{-n/2}$ where the maximum occurs at $\\hat{\\boldsymbol\\beta}_{(1)}=(\\mathbf Z_1^T\\mathbf Z_1)^{-1}\\mathbf Z_1^T\\boldsymbol y$  and $\\hat\\sigma_1^2=(\\mathbf y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})^T(\\mathbf y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})/n$ Reject $H_0:\\boldsymbol\\beta_{(2)}=\\boldsymbol0$ for small values of the **likelihood ratio test** : $$\\frac{\\underset{\\boldsymbol\\beta_{(1)},\\sigma^2}{\\text{max }}L(\\boldsymbol\\beta_{(1)},\\sigma^2)}{\\underset{\\boldsymbol\\beta,\\sigma^2}{\\text{max }}L(\\boldsymbol\\beta,\\sigma^2)}=\\Bigl(\\frac{\\hat\\sigma_1}{\\hat\\sigma}\\Bigr)^{-n}=\\Bigl(\\frac{\\hat\\sigma^2_1}{\\hat\\sigma^2}\\Bigr)^{-n/2}=\\Bigl(1+\\frac{\\hat\\sigma^2_1-\\hat\\sigma^2}{\\hat\\sigma^2}\\Bigr)^{-n/2}$$  Which is equivalent to rejecting $H_0$ when $\\frac{\\hat\\sigma^2_1-\\hat\\sigma^2}{\\hat\\sigma^2}$ is large or when  $$\\frac{n(\\hat\\sigma^2_1-\\hat\\sigma^2)/(r-q)}{n\\hat\\sigma^2/(n-r-1)}=\\frac{(SS_{error}(\\mathbf Z_1)-SS_{error}(\\mathbf Z))/(r-q)}{s^2}=F_{(r-q),(n-r-1)}$$ is large.   \n",
    "6) Let $y_0$ denote the response value when the predictor variables have values $\\mathbf z^T_0=[1,z_{01},z_{02},\\cdots,z_{0r}]$, then $E(y_0|\\mathbf z_0)=\\beta_0+z_{01}\\beta_1+\\cdots+z_{0r}\\beta_r=\\mathbf z^T_0\\boldsymbol\\beta$, its least squares estimate is $\\mathbf z^T_0\\hat{\\boldsymbol\\beta}$. Because $Cov(\\hat{\\boldsymbol\\beta})=\\sigma^2(\\mathbf Z^T\\mathbf Z)^{-1}$ and $\\hat{\\boldsymbol\\beta}$ is distributed as $N_{(r+1)}(\\boldsymbol\\beta,\\sigma^2(\\mathbf Z^T\\mathbf Z)^{-1})$, so $Var(\\mathbf z^T_0\\hat{\\boldsymbol\\beta})=\\mathbf z^T_0Cov(\\hat{\\boldsymbol\\beta})\\mathbf z_0=\\sigma^2\\mathbf z^T_0(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0$. Because $(n-r-1)s^2=\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}$ is distributed as $\\sigma^2\\chi_{n-r-1}^2$, so $\\frac{s^2}{\\sigma^2}=\\chi_{n-r-1}^2/(n-r-1)$. Consequently, the linear combination $\\mathbf z^T_0\\hat{\\boldsymbol\\beta}$ is distributed as $N(\\mathbf z^T_0\\boldsymbol\\beta,\\sigma^2\\mathbf z^T_0(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0)$ and $\\displaystyle\\frac{(\\mathbf z^T_0\\hat{\\boldsymbol\\beta}-\\mathbf z^T_0\\boldsymbol\\beta)/\\sqrt{\\sigma^2\\mathbf z^T_0(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0}}{\\sqrt{\\frac{s^2}{\\sigma^2}}}$ is distributed as $t_{n-r-1}$. Then a $100(1-\\alpha)\\%$ confidence interval for $E(y_0|\\mathbf z_0)=\\mathbf z^T_0\\boldsymbol\\beta$ is given by: $\\mathbf z^T_0\\hat{\\boldsymbol\\beta}\\pm t_{n-r-1}(\\frac{\\alpha}{2})\\sqrt{\\frac{s^2}{\\sigma^2}}\\sqrt{\\sigma^2\\mathbf z^T_0(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0}=\\mathbf z^T_0\\hat{\\boldsymbol\\beta}\\pm t_{n-r-1}(\\frac{\\alpha}{2})\\sqrt{s^2\\mathbf z^T_0(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0}$    \n",
    "\n",
    "\n",
    "* Multivariate linear regression:   \n",
    "When there are $m$ regression coefficients vectors $\\boldsymbol\\beta_1,\\cdots,\\boldsymbol\\beta_m$, each regression coefficients vector $\\boldsymbol\\beta$ has $r+1$ components $[\\beta_0,\\beta_1,\\cdots,\\beta_r]$, then the response variables are:\n",
    "$$\\underset{(n\\times m)}{\\underbrace{\\begin{bmatrix}\n",
    "Y_{11}&Y_{12}&\\cdots&Y_{1m}\\\\\n",
    "Y_{21}&Y_{22}&\\cdots&Y_{2m}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "Y_{n1}&Y_{n2}&\\cdots&Y_{nm}\\\\\n",
    "\\end{bmatrix}}}=\\underset{(n\\times (r+1))}{\\underbrace{\\begin{bmatrix}\n",
    "1&Z_{11}&Z_{12}&\\cdots&Z_{1r}\\\\\n",
    "1&Z_{21}&Z_{22}&\\cdots&Z_{2r}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "1&Z_{n1}&Z_{n2}&\\cdots&Z_{nr}\\\\\n",
    "\\end{bmatrix}}}\\underset{((r+1)\\times m)}{\\underbrace{\\begin{bmatrix}\n",
    "\\beta_{01}&\\beta_{02}&\\cdots&\\beta_{0m}\\\\\n",
    "\\beta_{11}&\\beta_{12}&\\cdots&\\beta_{1m}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "\\beta_{r1}&\\beta_{r2}&\\cdots&\\beta_{rm}\\\\\n",
    "\\end{bmatrix}}}+\\underset{(n\\times m)}{\\underbrace{\\begin{bmatrix}\n",
    "\\epsilon_{11}&\\epsilon_{12}&\\cdots&\\epsilon_{1m}\\\\\n",
    "\\epsilon_{21}&\\epsilon_{22}&\\cdots&\\epsilon_{2m}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "\\epsilon_{n1}&\\epsilon_{n2}&\\cdots&\\epsilon_{nm}\\\\\n",
    "\\end{bmatrix}}}$$  or\n",
    "$\\mathbf Y=\\mathbf Z\\boldsymbol\\beta+\\boldsymbol\\epsilon$, the $j^{th}$ row of $\\mathbf Y$ is the $m$ observations on the $j^{th}$ trial and regressed using $m$ regression coefficients vectors. The $i^{th}$ column of $\\mathbf Y$ is the $n$ observations on all of the $n$ trials and regressed using the $i^{th}$ regression coefficients vector. The $i^{th}$ column of $\\boldsymbol\\epsilon$ is the $n$ errors of the $n$ observations regressed using the $i^{th}$ regression coefficients vector $\\boldsymbol\\beta_i$, $\\mathbf Y_i=\\mathbf Z\\boldsymbol\\beta_i+\\boldsymbol\\epsilon_i$  \n",
    "The $2$ columns of $\\boldsymbol\\epsilon$, $\\boldsymbol\\epsilon_i$ and $\\boldsymbol\\epsilon_k$ are independent, with $E(\\boldsymbol\\epsilon_i)=\\boldsymbol0$ and $Cov(\\boldsymbol\\epsilon_i,\\boldsymbol\\epsilon_k)=\\sigma_{ik}\\mathbf I, \\quad i,(k=1,2,\\cdots,m)$. The $m$ observations on the $j^{th}$ trial with $2$ regression coefficients vectors $\\boldsymbol\\beta_i,\\boldsymbol\\beta_k$ have covariance matrix $\\underset{m\\times m}{\\boldsymbol\\Sigma}=\\{\\sigma_{ik}\\}$  \n",
    "\n",
    "* Like in the univariate linear regression models, $\\hat{\\boldsymbol\\beta}_i=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\mathbf Y_i$ and $\\hat{\\boldsymbol\\beta}=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\mathbf Y$  \n",
    "The Predicted values:$\\hat{\\mathbf Y}=\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\mathbf Y$,   \n",
    "Residuals: $\\hat{\\boldsymbol\\epsilon}=\\mathbf Y-\\hat{\\mathbf Y}=(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\mathbf Y$ also because $\\mathbf Z^T\\hat{\\boldsymbol\\epsilon}=\\mathbf Z^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\mathbf Y=\\mathbf0$, so the residuals $\\hat{\\boldsymbol\\epsilon}_i$ are perpendicular to the columns of $\\mathbf Z$, also because $\\hat{\\mathbf Y}^T\\hat{\\boldsymbol\\epsilon}=(\\mathbf Z\\hat{\\boldsymbol\\beta})^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\mathbf Y=\\hat{\\boldsymbol\\beta}^T\\mathbf Z^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\mathbf Y=\\mathbf0$, so $\\hat{\\mathbf Y}_i$ are perpendicular to all residual vectors $\\hat{\\boldsymbol\\epsilon}_i$ So $\\mathbf Y^T\\mathbf Y=(\\hat{\\mathbf Y}+\\hat{\\boldsymbol\\epsilon})^T(\\hat{\\mathbf Y}+\\hat{\\boldsymbol\\epsilon})=\\hat{\\mathbf Y}^T\\hat{\\mathbf Y}+\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}+\\mathbf0+\\mathbf0$ or $\\mathbf Y^T\\mathbf Y=\\hat{\\mathbf Y}^T\\hat{\\mathbf Y}+\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}$  \n",
    "\n",
    "1) Because $\\mathbf Y_i=\\mathbf Z\\boldsymbol\\beta_i+\\boldsymbol\\epsilon_i$, then $\\hat{\\boldsymbol\\beta}_i-\\boldsymbol\\beta_i=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\mathbf Y_i-\\boldsymbol\\beta_i=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T(\\mathbf Z\\boldsymbol\\beta_i+\\boldsymbol\\epsilon_i)-\\boldsymbol\\beta_i=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol\\epsilon_i$ so $E(\\hat{\\boldsymbol\\beta}_i-\\boldsymbol\\beta_i)=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^TE(\\boldsymbol\\epsilon_i)=\\mathbf0$, or $E(\\hat{\\boldsymbol\\beta}_i)=\\boldsymbol\\beta_i$\n",
    "and because $\\hat{\\boldsymbol\\epsilon}_i=\\mathbf Y_i-\\hat{\\mathbf Y}_i=(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\mathbf Y_i=(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)(\\mathbf Z\\boldsymbol\\beta_i+\\boldsymbol\\epsilon_i)=(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol\\epsilon_i$, so $E(\\hat{\\boldsymbol\\epsilon}_i)=\\mathbf0$  And  $$\\begin{align}\n",
    "Cov(\\hat{\\boldsymbol\\beta}_i,\\hat{\\boldsymbol\\beta}_k)&=E(\\hat{\\boldsymbol\\beta}_i-\\boldsymbol\\beta_i)(\\hat{\\boldsymbol\\beta}_k-\\boldsymbol\\beta_k)^T\\\\\n",
    "&=E((\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol\\epsilon_i)((\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol\\epsilon_k)^T\\\\\n",
    "&=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^TE(\\boldsymbol\\epsilon_i\\boldsymbol\\epsilon_k^T)\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\\\\n",
    "&=\\sigma_{ik}(\\mathbf Z^T\\mathbf Z)^{-1}\n",
    "\\end{align}$$ Then $$\\begin{align}\n",
    "E(\\hat{\\boldsymbol\\epsilon}_i^T\\hat{\\boldsymbol\\epsilon}_k)&=E\\Biggl(((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol\\epsilon_i)^T((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol\\epsilon_k)\\Biggr)\\\\\n",
    "&=E(\\boldsymbol\\epsilon_i^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol\\epsilon_k)\\\\\n",
    "&=E(tr(\\boldsymbol\\epsilon_i^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol\\epsilon_k))\\\\\n",
    "&=E(tr((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol\\epsilon_k\\boldsymbol\\epsilon_i^T))\\\\\n",
    "&=tr[(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)E(\\boldsymbol\\epsilon_k\\boldsymbol\\epsilon_i^T)]\\\\\n",
    "&=tr[(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\sigma_{ik}\\mathbf I]\\\\\n",
    "&=\\sigma_{ik}tr[\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T]\\\\\n",
    "&=\\sigma_{ik}(n-r-1)\n",
    "\\end{align}$$\n",
    "$$\\begin{align}\n",
    "Cov(\\hat{\\boldsymbol\\beta}_i,\\hat{\\boldsymbol\\epsilon}_k)&=E(\\hat{\\boldsymbol\\beta}_i-E(\\hat{\\boldsymbol\\beta}_i))(\\hat{\\boldsymbol\\epsilon}_k-E(\\hat{\\boldsymbol\\epsilon}_k))^T\\\\\n",
    "&=E(\\hat{\\boldsymbol\\beta}_i-\\boldsymbol\\beta_i)(\\hat{\\boldsymbol\\epsilon}_k-\\boldsymbol\\epsilon_k)^T\\\\\n",
    "&=E((\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol\\epsilon_i)(\\hat{\\boldsymbol\\epsilon}_k)^T\\\\\n",
    "&=E((\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol\\epsilon_i)((\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\boldsymbol\\epsilon_k)^T\\\\\n",
    "&=E((\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\boldsymbol\\epsilon_i)(\\boldsymbol\\epsilon_k^T(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T))\\\\\n",
    "&=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^TE(\\boldsymbol\\epsilon_i\\boldsymbol\\epsilon_k^T)(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T))\\\\\n",
    "&=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\sigma_{ik}\\mathbf I(\\mathbf 1-\\mathbf Z(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T))\\\\\n",
    "&=\\sigma_{ik}((\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T-(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T)\\\\\n",
    "&=\\mathbf0\n",
    "\\end{align}$$so each element of $\\hat{\\boldsymbol\\beta}$ is uncorrelated with each element of $\\hat{\\boldsymbol\\epsilon}$.   \n",
    "\n",
    "2) $\\hat{\\boldsymbol\\beta}=(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf Z^T\\mathbf Y$ is the maximum likelihood estimator of $\\boldsymbol\\beta$ and $\\hat{\\boldsymbol\\beta}$ is a normal distribution with $E(\\hat{\\boldsymbol\\beta})=\\boldsymbol\\beta$ and $Cov(\\hat{\\boldsymbol\\beta}_i,\\hat{\\boldsymbol\\beta}_k)=\\sigma_{ik}(\\mathbf Z^T\\mathbf Z)^{-1}$.   And $\\hat{\\boldsymbol\\Sigma}=\\frac{1}{n}\\hat{\\boldsymbol\\epsilon}^T\\hat{\\boldsymbol\\epsilon}=\\frac{1}{n}(\\mathbf Y-\\mathbf Z\\hat{\\boldsymbol\\beta})^T(\\mathbf Y-\\mathbf Z\\hat{\\boldsymbol\\beta})$ is the maximum likelihood estimator of $\\boldsymbol\\Sigma$ and $n\\hat{\\boldsymbol\\Sigma}$ is distribution with $W_{p,n-r-1}(\\boldsymbol\\Sigma)$. The likelihood associated with the parameters $\\boldsymbol\\beta$ and $\\boldsymbol\\Sigma$ is $$\\begin{align}\n",
    "L(\\hat{\\boldsymbol\\mu},\\hat{\\boldsymbol\\Sigma})&=\\prod_{j=1}^{n}\\Biggl[\\frac{1}{(2\\pi)^{m/2}|\\hat{\\boldsymbol\\Sigma}|^{1/2}}e^{-\\frac{1}{2}(\\mathbf y_j-\\boldsymbol\\mu)^T\\hat{\\boldsymbol\\Sigma}^{-1}(\\mathbf y_j-\\boldsymbol\\mu)}\\Biggr]\\\\\n",
    "&=\\frac{1}{(2\\pi)^{\\frac{mn}{2}}|\\hat{\\mathbf\\Sigma}|^{n/2}}e^{-\\frac{1}{2}\\sum_{j=1}^{n}(\\mathbf y_j-\\boldsymbol\\mu)^T\\hat{\\boldsymbol\\Sigma}^{-1}(\\mathbf y_j-\\boldsymbol\\mu)}\\\\\n",
    "&=\\frac{1}{(2\\pi)^{\\frac{mn}{2}}|\\hat{\\boldsymbol\\Sigma}|^{n/2}}e^{-\\frac{1}{2}tr\\Biggl[\\hat{\\boldsymbol\\Sigma}^{-1}\\sum_{j=1}^{n}(\\mathbf y_j-\\boldsymbol\\mu)^T(\\mathbf y_j-\\boldsymbol\\mu)\\Biggr]}\\\\\n",
    "&=\\frac{1}{(2\\pi)^{(mn/2)}|\\hat{\\boldsymbol\\Sigma}|^{n/2}}e^{-\\frac{1}{2}mn}\n",
    "\\end{align}$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) If some of $\\mathbf Z=[z_0,z_1,z_2,\\cdots,z_q,z_{q+1},z_{q+2},\\cdots,z_{r}]$ parameters $[z_{q+1},z_{q+2},\\cdots,z_{r}]$ do not influence $\\mathbf Y$, which means the hypothesis $H_0:\\boldsymbol\\beta_{q+1}=\\boldsymbol\\beta_{q+2}=\\cdots=\\boldsymbol\\beta_{r}=\\boldsymbol0$. We can express the general linear model as $$\\mathbf Y=\\mathbf Z\\boldsymbol\\beta+\\boldsymbol\\epsilon=\\left[\\begin{array}{c:c}\n",
    "\\underset{n\\times(q+1)}{\\mathbf Z_1}&\\underset{n\\times(r-q)}{\\mathbf Z_2}\n",
    "\\end{array}\n",
    "\\right]\\begin{bmatrix}\n",
    "\\underset{(q+1)\\times m}{\\boldsymbol\\beta_{(1)}}\\\\\n",
    "\\hdashline\n",
    "\\underset{(r-q)\\times m}{\\boldsymbol\\beta_{(2)}}\\\\\n",
    "\\end{bmatrix}+\\boldsymbol\\epsilon=\\mathbf Z_1\\boldsymbol\\beta_{(1)}+\\mathbf Z_2\\boldsymbol\\beta_{(2)}+\\boldsymbol\\epsilon$$ Now the hypothesis is $H_0:\\boldsymbol\\beta_{(2)}=\\boldsymbol0,\\mathbf Y=\\mathbf Z_1\\boldsymbol\\beta_{(1)}+\\boldsymbol\\epsilon$ The $$\\begin{align}\n",
    "\\text{Extra sum of squares}&=SS_{\\text{error}}(\\mathbf Z_1)-SS_{\\text{error}}(\\mathbf Z)\\\\\n",
    "&=(\\mathbf Y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})^T(\\mathbf Y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})-(\\mathbf Y-\\mathbf Z\\hat{\\boldsymbol\\beta})^T(\\mathbf Y-\\mathbf Z\\hat{\\boldsymbol\\beta})\n",
    "\\end{align}$$ where $\\hat{\\boldsymbol\\beta}_{(1)}=(\\mathbf Z_1^T\\mathbf Z_1)^{-1}\\mathbf Z_1^T\\mathbf Y$. And $$\\hat{\\boldsymbol\\Sigma}_1=\\frac{1}{n}SS_{\\text{error}}(\\mathbf Z_1)=\\frac{1}{n}(\\mathbf Y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})^T(\\mathbf Y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})$$  Under the restriction of the null hypothesis, the maximum likelihood is $\\underset{\\boldsymbol\\beta_{(1)},\\boldsymbol\\Sigma_1}{\\text{max }}L(\\boldsymbol\\beta_{(1)},\\boldsymbol\\Sigma_1)=\\displaystyle\\frac{1}{(2\\pi)^{mn/2}\\hat{\\boldsymbol\\Sigma}_1^{n/2}}e^{-mn/2}$ where the maximum occurs at $\\hat{\\boldsymbol\\beta}_{(1)}=(\\mathbf Z_1^T\\mathbf Z_1)^{-1}\\mathbf Z_1^T\\boldsymbol Y$  and $\\hat{\\boldsymbol\\Sigma}_1=\\frac{1}{n}(\\mathbf Y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})^T(\\mathbf Y-\\mathbf Z_1\\hat{\\boldsymbol\\beta}_{(1)})$ Reject $H_0:\\boldsymbol\\beta_{(2)}=\\boldsymbol0$ for small values of the **likelihood ratio test** : $$\\frac{\\underset{\\boldsymbol\\beta_{(1)},\\boldsymbol\\Sigma_1}{\\text{max }}L(\\boldsymbol\\beta_{(1)},\\boldsymbol\\Sigma_1)}{\\underset{\\boldsymbol\\beta,\\boldsymbol\\Sigma}{\\text{max }}L(\\boldsymbol\\beta,\\boldsymbol\\Sigma)}=\\Bigl(\\frac{|\\hat{\\boldsymbol\\Sigma}_1|}{|\\hat{\\boldsymbol\\Sigma}|}\\Bigr)^{-n/2}=\\Bigl(\\frac{|\\hat{\\boldsymbol\\Sigma}|}{|\\hat{\\boldsymbol\\Sigma}_1|}\\Bigr)^{n/2}=\\Lambda$$, where $\\Lambda^{2/n}=\\frac{|\\hat{\\boldsymbol\\Sigma}|}{|\\hat{\\boldsymbol\\Sigma}_1|}$ is the <span style=\"color: red;\">**Wilks’ lambda statistic**</span>.  \n",
    "\n",
    "4) The responses $\\mathbf y_0$ corresponding to fixed values $\\mathbf z_0$ of the predictor variables and regression coefficients metrix $\\underset{n\\times m}{\\boldsymbol\\beta}$ is $\\mathbf y_0=\\mathbf z_0^T\\hat{\\boldsymbol\\beta}+\\boldsymbol\\epsilon_0$, $E(\\hat{\\boldsymbol\\beta}^T\\mathbf z_0)=\\boldsymbol\\beta^T\\mathbf z_0$ and $Cov(\\hat{\\boldsymbol\\beta}^T\\mathbf z_0)=\\mathbf z_0^TCov(\\hat{\\boldsymbol\\beta}^T)\\mathbf z_0=\\mathbf z_0^T(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0\\boldsymbol\\Sigma$, so $\\underset{m\\times 1}{(\\hat{\\boldsymbol\\beta}^T\\mathbf z_0)}$ is distributed as $N_m(\\boldsymbol\\beta^T\\mathbf z_0,\\mathbf z_0^T(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0\\boldsymbol\\Sigma)$ and $n\\hat{\\boldsymbol\\Sigma}$ is independently distributed as $W_{n-r-1}(\\boldsymbol\\Sigma)$. So the $T^2$-statistic is $$T^2=\\Biggl(\\frac{\\hat{\\boldsymbol\\beta}^T\\mathbf z_0-\\boldsymbol\\beta^T\\mathbf z_0}{\\sqrt{\\mathbf z_0^T(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0}}\\Biggr)^T\\Biggl(\\frac{n\\hat{\\boldsymbol\\Sigma}}{n-r-1}\\Biggr)^{-1}\\Biggl(\\frac{\\hat{\\boldsymbol\\beta}^T\\mathbf z_0-\\boldsymbol\\beta^T\\mathbf z_0}{\\sqrt{\\mathbf z_0^T(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0}}\\Biggr)$$ and $100(1-\\alpha)\\%$ confidence ellipsoid for $\\boldsymbol\\beta^T\\mathbf z_0$ is provided by the inequality $$T^2\\le\\frac{m(n-r-1)}{n-r-m}F_{m,n-r-m}(\\alpha)$$ The $100(1-\\alpha)\\%$ simultaneous confidence intervals for the component of $\\mathbf z_0^T\\boldsymbol\\beta$, $\\mathbf z_0^T\\boldsymbol\\beta_i$ is given by: $$\\mathbf z_0^T\\hat{\\boldsymbol\\beta_i}\\pm\\sqrt{\\frac{m(n-r-1)}{n-r-m}F_{m,n-r-m}(\\alpha)}\\sqrt{\\mathbf z_0^T(\\mathbf Z^T\\mathbf Z)^{-1}\\mathbf z_0}\\sqrt{\\frac{n\\hat{\\sigma_{ii}}}{n-r-1}}\n",
    "$$ $i=1,2,\\cdots,m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.9**. Under appropriate conditions we have:\n",
    "\n",
    "1. (Consistency) $\\hat{\\beta}_0 \\xrightarrow{\\text{P}} \\beta_0$ and $\\hat{\\beta}_1 \\xrightarrow{\\text{P}} \\beta_1$\n",
    "\n",
    "2. (Asymptotic Normality):\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_0 - \\beta_0}{\\hat{se}(\\hat{\\beta}_0)} \\leadsto N(0, 1)\n",
    "\\quad \\text{and} \\quad\n",
    "\\frac{\\hat{\\beta}_1 - \\beta_1}{\\hat{se}(\\hat{\\beta}_1)} \\leadsto N(0, 1)\n",
    "$$\n",
    "\n",
    "3. Approximate $1 - \\alpha$ confidence intervals for $\\beta_0$ and $\\beta_1$ are\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\beta}_0)\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\beta}_1 \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\beta}_1)\n",
    "$$\n",
    "\n",
    "The Wald statistic for testing $H_0 : \\beta_1 = 0$ versus $H_1: \\beta_1 \\neq 0$ is: reject $H_0$ if $W > z_{\\alpha / 2}$ where $W = \\hat{\\beta}_1 / \\hat{\\text{se}}(\\hat{\\beta}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have estimated a regression model $\\hat{r}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ from data $(X_1, Y_1), \\dots, (X_n, Y_n)$.  We observe the value $X_* = x$ of the covariate for a new subject and we want to predict the outcome $Y_*$.  An estimate of $Y_*$ is \n",
    "\n",
    "$$ \\hat{Y}_* = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_*$$\n",
    "\n",
    "Using the formula for the variance of the sum of two random variables,\n",
    "\n",
    "$$ \\mathbb{V}(\\hat{Y}_*) = \\mathbb{V}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_*) = \\mathbb{V}(\\hat{\\beta}_0) + x_* \\mathbb{V}(\\hat{\\beta}_1) + 2 x_* \\text{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) $$ \n",
    "\n",
    "Theorem 14.8 gives the formulas for all terms in this equation.  The estimated standard error $\\hat{\\text{se}}(\\hat{Y}_*)$ is the square root of this variance, with $\\hat{\\sigma}^2$ in place of $\\sigma^2$.  However, **the confidence interval for $\\hat{Y}_*$ is not of the usual form** $\\hat{Y}_* \\pm z_{\\alpha} \\hat{\\text{se}}(\\hat{Y}_*)$.  The appendix explains why.  The correct form is given in the following theorem.  We can the interval a **prediction interval**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.11 (Prediction Interval)**.  Let\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\xi}_n^2 &= \\hat{\\text{se}}^2(\\hat{Y}_*) + \\hat{\\sigma}^2 \\\\\n",
    "&= \\hat{\\sigma}^2 \\left(\\frac{\\sum_{i=1}^n (X_i - X_*)^2}{n \\sum_{i=1}^n (X_i - \\overline{X})^2} + 1 \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "An approximate $1 - \\alpha$ prediction interval for $Y_*$ is\n",
    "\n",
    "$$ \\hat{Y}_* \\pm z_{\\alpha/2} \\xi_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.5 Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have $k$ covariates $X_1, \\dots, X_k$.  The data are of the form\n",
    "\n",
    "$$(Y_1, X_1), \\dots, (Y_i, X_i), \\dots, (Y_n, X_n)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ X_i = (X_{i1}, \\dots, X_{ik}) $$\n",
    "\n",
    "Here, $X_i$ is the vector of $k$ covariates for the $i-$th observation.  The linear regression model is\n",
    "\n",
    "$$ Y_i = \\sum_{j=1}^k \\beta_j X_{ij} + \\epsilon_i $$\n",
    "\n",
    "for $i = 1, \\dots, n$ where $\\mathbb{E}(\\epsilon_i | X_{1i}, \\dots, X_{ik}) = 0$.  Usually we want to include an intercept in the model which we can do by setting $X_{i1} = 1$ for $i = 1, \\dots, n$.  At this point it will become more convenient to express the model in matrix notation.  The outcomes will be denoted by\n",
    "\n",
    "$$ Y = \\begin{pmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and the covariates will be denoted by\n",
    "\n",
    "$$ X = \\begin{pmatrix}\n",
    "X_{11} & X_{12} & \\cdots & X_{1k} \\\\\n",
    "X_{21} & X_{22} & \\cdots & X_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "X_{n1} & X_{n2} & \\cdots & X_{nk}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Each row is one observation; the columns represent to the $k$ covariates.  Thus, $X$ is a $n \\times k$ matrix.  Let\n",
    "\n",
    "$$\n",
    "\\beta = \\begin{pmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_k\n",
    "\\end{pmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\epsilon = \\begin{pmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_k\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then we can write the linear regression model as\n",
    "\n",
    "$$ Y = X \\beta + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.13**. Assuming that the $k \\times k$ matrix $X^TX$ is invertible, the least squares estimate is\n",
    "\n",
    "$$ \\hat{\\beta} = (X^T X)^{-1} X^T Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated regression function is\n",
    "\n",
    "$$ \\hat{r}(x) = \\sum_{j=1}^k \\hat{\\beta}_j x_j$$\n",
    "\n",
    "The variance-covariance matrix of $\\hat{\\beta}$ is\n",
    "\n",
    "$$ \\mathbb{V}(\\hat{\\beta} | X^n) = \\sigma^2 (X^T X)^{-1} $$\n",
    "\n",
    "An unbiased estimate of $\\sigma^2$ is\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\left( \\frac{1}{n - k} \\right) \\sum_{i=1}^n \\hat{\\epsilon}_i^2 $$\n",
    "\n",
    "where $\\hat{\\epsilon} = X \\hat{\\beta} - Y$ is the vector of residuals.  An approximate $1 - \\alpha$ confidence interval for $\\beta_j$ is\n",
    "\n",
    "$$ \\hat{\\beta}_j \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\beta}_j) $$\n",
    "\n",
    "where $\\hat{\\text{se}}^2(\\hat{\\beta}_j)$ is the $j$-th diagonal element of the matrix $\\hat{\\sigma}^2 (X^T X)^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.6 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have data on many covariates but we may not want to include all of them in the model.  A smaller model with fewer covariates has two advantages: it might give better predictions than a big model and it is more parsimonious (simpler).  Generally, as you add more variables to a regression, the bias of the predictions decreases and the variance increases.  Too few covariates yields high bias; too many covariates yields high variance.  Good predictions result from achieving a good balance between bias and variance.\n",
    "\n",
    "In model selection there are two problems: assigning a score to each model which measures, in some sense, how good the model is, and searching through all models to find the model with the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S \\subset \\{1, \\dots, k\\}$ and let $\\mathcal{X}_S = \\{ X_j : j \\in S \\}$ denote a subset of the covariates.  Let $\\beta_S$ denote the coefficients of the corresponding set of covariates and let $\\hat{\\beta}_S$ denote the least squares estimate of $\\beta_S$.  Let $X_S$ denote the $X$ matrix for this subset of covariates, and let $\\hat{r}_S(x)$ to be the estimated regression function.  The predicted values from model $S$ are denoted by $\\hat{Y}_i(S) = \\hat{r}_S(X_i)$.  \n",
    "\n",
    "The **prediction risk** is defined to be\n",
    "\n",
    "$$R(S) = \\sum_{i=1}^n \\mathbb{E} (\\hat{Y}_i(S) - Y_i^*)^2 $$\n",
    "\n",
    "where $Y_i^*$ denotes the value of the future observation of $Y_i$ at covariate value $X_i$.  Our goal is to choose $S$ to make $R(S)$ small.\n",
    "\n",
    "The **training error** is defined to be\n",
    "\n",
    "$$\\hat{R}_\\text{tr}(S) = \\sum_{i=1}^n (\\hat{Y}_i(S) - Y_i)^2 $$\n",
    "\n",
    "This estimate is very biased and under-estimates $R(S)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.15**.  The training error is a downward biased estimate of the prediction risk:\n",
    "\n",
    "$$ \\mathbb{E}(\\hat{R}_\\text{tr}(S)) < R(S) $$\n",
    "\n",
    "In fact,\n",
    "\n",
    "$$\\text{bias}(\\hat{R}_\\text{tr}(S)) = \\mathbb{E}(\\hat{R}_\\text{tr}(S)) - R(S) = -2 \\sum_{i=1}^n \\text{Cov}(\\hat{Y}_i, Y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the bias is that the data is being used twice: to estimate the parameters and to estimate the risk.  When fitting a model with many variables, the covariance $\\text{Cov}(\\hat{Y_i}, Y_i)$ will be large and the bias of the training error gets worse.\n",
    "\n",
    "In summary, the training error is a poor estimate of risk.  Here are some better estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mallow's $C_p$ statistic** is defined by\n",
    "\n",
    "$$\\hat{R}(S) = \\hat{R}_\\text{tr}(S) + 2 |S| \\hat{\\sigma}^2$$\n",
    "\n",
    "where $|S|$ denotes the number of terms in $S$ and $\\hat{\\sigma}^2$ is the estimate of $\\sigma^2$ obtained from the full model (with all covariates).  Think of the $C_p$ statistic as lack of fit plus complexity penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related method for estimating risk is **AIC (Akaike Information Criterion)**.  The idea is to choose $S$ to maximize\n",
    "\n",
    "$$ \\ell_S - |S|$$\n",
    "\n",
    "where $\\ell_S$ is the log-likelihood of the model evaluated at the MLE.  In linear regression with Normal errors, maximizing AIC is equivalent to minimizing Mallow's $C_p$; see exercise 8.\n",
    "\n",
    "*Some texts use a slightly different definition of AIC which involves multiplying this definition by 2 or -2.  This has no effect on which model is selected.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another method for estimating risk is **leave-one-out cross-validation**.  In this case, the risk estimator is\n",
    "\n",
    "$$\\hat{R}_\\text{CV}(S) = \\sum_{i=1}^n (Y_i - \\hat{Y}_{(i)})^2 $$\n",
    "\n",
    "where $\\hat{Y}_{(i)}$ is the prediction for $Y_i$ obtained by fitting the model with $Y_i$ omitted.  It can be shown that\n",
    "\n",
    "$$\\hat{R}_\\text{CV}(S) = \\sum_{i=1}^n \\left( \\frac{Y_i - \\hat{Y}_i(S)}{1 - U_{ii}(S)} \\right)^2 $$\n",
    "\n",
    "where $U_ii$ is the $i$-th diagonal element of the matrix\n",
    "\n",
    "$$U(S) = X_S (X_S^T X_S)^{-1} X_S^T$$\n",
    "\n",
    "Thus one need not actually drop each observation and re-fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generalization is **k-fold cross-validation**.  Here we divide the data into $k$ groups; often people take $k = 10$.  We omit one group of data and fit the models on the remaining data.  We use the fitted model to predict the data in the group that was omitted.  We then estimate the risk by $\\sum_i (Y_i - \\hat{Y}_i)^2$ where the sum is over the data points in the omitted group.  This process is repeated for each of the $k$ groups and the resulting risk estimates are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression, Mallows $C_p$ and cross-validation often yield essentially the same results so one might as well use Mallow's method.  In some of the more complex problems we will discuss later, cross-validation will be more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another scoring method is **BIC (Bayesian Information Criterion)**.  Here we choose a model to maximize\n",
    "\n",
    "$$ \\text{BIC}(S) = \\text{RSS}(S) = 2 |S| \\hat{\\sigma}^2 $$\n",
    "\n",
    "The BIC score has a Bayesian interpretation.  Let $\\mathcal{S} = \\{ S_1, \\dots, S_m \\}$ denote a set of models.  Suppose we assign the uniform prior $\\mathbb{P}(S_j) = 1 / m$ over the models.  Also assume we put a smooth prior on the parameters within each model.  It can be shown that the posterior probability for a model is approximately\n",
    "\n",
    "$$ \\mathbb{P}(S_j | \\text{data}) \\approx \\frac{e^{\\text{BIC}(S_j)}}{\\sum_r e^{\\text{BIC}(S_r)}}$$\n",
    "\n",
    "so choosing the model with highest BIC is like choosing the model with highest posterior probability.\n",
    "\n",
    "The BIC score also has an information-theoretical interpretation in terms of something called minimum description length.\n",
    "\n",
    "The BIC score is identical to Mallows $C_p$ except that it puts a more severe penalty for complexity.  It thus leads one to choose a smaller model than the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are $k$ covariates then there are $2^k$ possible models. We need to search through all of those models, assign a score to each one, and choose the model with the best score.  When $k$ is large, this is infeasible; in that case, we need to search over a subset of all the models.  Two common methods are **forward and backward stepwise regression**.\n",
    "\n",
    "In forward stepwise regression, we start with no covariates in the model, and keep adding variables one at a time that lead to the best score.  In backward stepwise regression, we start with the biggest model (all covariates) and drop one variable at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.7 The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method, due to Tibshirani, is called the **Lasso**. Assume that all covariates have been rescaled to have the same variance.  Consider estimating $\\beta = (\\beta_1, \\dots, \\beta_k)$ by minimizing the loss function\n",
    "\n",
    "$$ \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^k | \\beta_j |$$\n",
    "\n",
    "where $\\lambda > 0$.  The idea is to minimize the sums of squares but there is a penalty that gets large if any $\\beta_j$ gets large.  It can be shown that some of the $\\beta_j$'s will be 0.  We interpret this as having the $j$-th covariate omitted from the model; thus we are doing estimation and model selection simultaneously.\n",
    "\n",
    "We need to choose a value of $\\lambda$.  We can do this by estimating the prediction risk $R(\\lambda)$ as a function of $\\lambda$ and choosing to minimize it.  For example, we can estimate the risk using leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.8 Technical Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction interval is of a different form than other confidence intervals we have seen -- the quantity of interest  $Y_*$ is equal to a parameter $\\theta$ plus a random variable.\n",
    "\n",
    "We can fix this by defining:\n",
    "\n",
    "$$ \\xi_n^2 = \\mathbb{V}(\\hat{Y}_*) + \\sigma^2 = \\left[\\frac{\\sum_i (x_i - x_*)^2}{n \\sum_i (x_i - \\overline{x})^2} + 1\\right] \\sigma^2$$\n",
    "\n",
    "In practice, we substitute $\\hat{\\sigma}$ for $\\sigma$ and we denote the resulting quantity by $\\hat{\\xi}_n$.  Now,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(\\hat{Y}_* - z_{\\alpha/2} \\hat{\\xi}_n < Y_* < \\hat{Y}_* + z_{\\alpha/2} \\hat{\\xi}_n) &=\n",
    "\\mathbb{P}\\left(-z_{\\alpha/2} < \\frac{\\hat{Y}_* - Y_*}{\\hat{\\xi}_n} < z_{\\alpha/2} \\right)\\\\\n",
    "&= \\mathbb{P}\\left(-z_{\\alpha/2} < \\frac{\\hat{\\theta} - \\theta - \\epsilon}{\\hat{\\xi}_n} < z_{\\alpha/2} \\right) \\\\\n",
    "&\\approx \\mathbb{P}\\left(-z_{\\alpha/2} < \\frac{N(0, s^2 + \\sigma^2)}{\\hat{\\xi}_n} < z_{\\alpha/2} \\right)  \\\\\n",
    "&= \\mathbb{P}(-z_{\\alpha/2} < N(0, 1) < z_{\\alpha/2}) \\\\\n",
    "&= 1 - \\alpha\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.9 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.1**.  Prove Theorem 14.4:\n",
    "\n",
    "The least square estimates are given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (X_i - \\overline{X}_n) (Y_i - \\overline{Y}_n)}{\\sum_{i=1}^n (X_i - \\overline{X}_n)^2}\\\\\n",
    "\\hat{\\beta}_0 &= \\overline{Y}_n - \\hat{\\beta}_1 \\overline{X}_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "An unbiased estimate of $\\sigma^2$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\left( \\frac{1}{n - 2} \\right) \\sum_{i=1}^n \\hat{\\epsilon}_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We can obtain the estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ by minimizing the RSS -- by taking the partial derivatives with respect to $\\beta_0$ and $\\beta_1$:\n",
    "\n",
    "$$\\text{RSS} = \\sum_i \\hat{\\epsilon}_i^2 = \\sum_i (Y_i - (\\beta_0 + \\beta_1 X_i))^2$$\n",
    "\n",
    "Derivating RSS on $\\beta_0$:\n",
    "\n",
    "$$\\frac{d}{d \\beta_0}\\text{RSS} = \\sum_i \\frac{d}{d \\beta_0} (Y_i - (\\beta_0 + \\beta_1 X_i))^2\n",
    "= \\sum_i 2 (\\beta_0 - (Y_i - \\beta_1 X_i))$$\n",
    "\n",
    "Making this derivative equal to 0 at $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ gives:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "0 &= \\sum_i 2 (\\hat{\\beta}_0 - (Y_i - \\hat{\\beta}_1 X_i))\\\\\n",
    "n \\hat{\\beta}_0 &=  \\sum_i Yi - \\hat{\\beta}_1  \\sum_i X_i\\\\\n",
    "\\hat{\\beta}_0 &= \\overline{Y}_n - \\hat{\\beta}_1 \\overline{X}_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Replacing $\\overline{Y}_n - \\beta_1 \\overline{X}_n$ for $\\beta_0$ and derivating on $\\beta_1$:\n",
    "\n",
    "$$\\frac{d}{d \\beta_1}\\text{RSS} = \\sum_i \\frac{d}{d \\beta_1} (Y_i - (\\beta_0 + \\beta_1 X_i))^2\\\\\n",
    "=\\sum_i \\frac{d}{d \\beta_1} (Y_i - \\overline{Y}_n - \\beta_1 (X_i - \\overline{X}_n)))^2\n",
    "= \\sum_i -2 (X_i - \\overline{X}_n) (Y_i - \\overline{Y}_n - \\beta_1 (X_i - \\overline{X}_n)))$$\n",
    "\n",
    "Making this derivative equal to 0 at $\\hat{\\beta}_1$ gives:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "0 &= \\sum_i (X_i - \\overline{X}_n) (Y_i - \\overline{Y}_n - \\beta_1 (X_i - \\overline{X}_n))) \\\\\n",
    "0 &= \\hat{\\beta}_1 \\sum_i (\\overline{X}_n - X_i)^2 + \\sum_i (\\overline{X}_n - X_i)(Y_i - \\overline{Y}_n) \\\\\n",
    "\\hat{\\beta}_1 &= \\frac{\\sum_i (X_i - \\overline{X}_n)(Y_i - \\overline{Y}_n)}{\\sum_i (X_i - \\overline{X}_n)^2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the unbiased estimate, let's adapt a more general proof from Greene (2003), restricted to $k = 2$ dimensions, where the first dimension is set to all ones to represent the intercept, and the second dimension represents the one-dimensional covariates $X_i$.\n",
    "\n",
    "The vector of least square residuals is\n",
    "\n",
    "$$ \\hat{\\epsilon} = \\begin{pmatrix}\n",
    "\\hat{\\epsilon}_1 \\\\\n",
    "\\hat{\\epsilon}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{\\epsilon}_n\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "Y_1 - (\\hat{\\beta}_0 \\cdot 1 + \\hat{\\beta}_1 X_1) \\\\\n",
    "Y_2 - (\\hat{\\beta}_0 \\cdot 1 + \\hat{\\beta}_1 X_2) \\\\\n",
    "\\vdots \\\\\n",
    "Y_n - (\\hat{\\beta}_0 \\cdot 1 + \\hat{\\beta}_1 X_n)\n",
    "\\end{pmatrix} = \n",
    "y - X \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "where $$\n",
    "y = \\begin{pmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{pmatrix}\n",
    ", \\quad\n",
    "X = \\begin{pmatrix}\n",
    "1 & X_1 \\\\\n",
    "1 & X_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & X_n\n",
    "\\end{pmatrix},\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\beta} = \\begin{pmatrix}\n",
    "\\hat{\\beta}_0 \\\\\n",
    "\\hat{\\beta}_1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The least squares solution can be written as:\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Replacing it on the definition of $\\hat{\\epsilon}$, we get\n",
    "\n",
    "$$ \\hat{\\epsilon} = y - X (X^T X)^{-1} X^T y = (I - X (X^T X)^{-1} X^T) y = M y$$\n",
    "\n",
    "where $M = I - X (X^T X)^{-1} X^T$ is known as the **residual maker** matrix.\n",
    "\n",
    "Note that $M$ is symmetric, that is, $M^T = M$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "M^T &= (I - X (X^T X)^{-1} X^T)^T  \\\\\n",
    "&= I^T - (X (X^T X)^{-1} X^T)^T \\\\\n",
    "&= I - (X^T)^T ((X^T X)^{-1})^T X^T \\\\\n",
    "&= I - X ((X^{-1}(X^T)^{-1})^T X^T \\\\\n",
    "&= I -  X (X^T X)^{-1} X^T \\\\\n",
    "&= M\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note also that $M$ is idempotent, that is, $M^2 = M$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "M^2 &= (I - X (X^T X)^{-1} X^T) (I - X (X^T X)^{-1} X^T) \\\\\n",
    "&= I - X (X^T X)^{-1} X^T - X (X^T X)^{-1} X^T + X \\left( (X^T X)^{-1} X^T X \\right) (X^T X)^{-1} X^T \\\\\n",
    "&= I - X (X^T X)^{-1} X^T - X (X^T X)^{-1} X^T + X (X^T X)^{-1} X^T \\\\\n",
    "&= I - X (X^T X)^{-1} X^T \\\\\n",
    "&= M\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we have that $MX = 0$, as running least squares on a regression where the covariates match the target variables should yield a model that just copies the covariate over, where all residuals are zero ($\\beta_0 = 0$ and $\\beta_1 = 1$).  So:\n",
    "\n",
    "$$\\hat{\\epsilon} = M y = M( X \\beta + \\epsilon) = M\\epsilon$$\n",
    "\n",
    "where $\\epsilon = Y - X\\beta$ are the population residuals.\n",
    "\n",
    "We can then write an estimator for $\\sigma^2$:\n",
    "\n",
    "$$ \\hat{\\epsilon}^T \\hat{\\epsilon} = \\epsilon^T M^T M \\epsilon = \\epsilon^T M^2 \\epsilon = \\epsilon^T M \\epsilon$$\n",
    "\n",
    "Taking the expectation with respect to the data $X$ on both sides,\n",
    "\n",
    "$$\\mathbb{E}(\\hat{\\epsilon}^T \\hat{\\epsilon} | X) = \\mathbb{E}(\\epsilon^T M \\epsilon | X)$$\n",
    "\n",
    "But $\\epsilon^T M \\epsilon$ is a scalar ($1 \\times 1$ matrix), so it is equal to its trace -- and we can use the cyclic permutation property of the trace:\n",
    "\n",
    "$$ \\mathbb{E}(\\epsilon^T M \\epsilon | X) = \\mathbb{E}(\\text{tr}(\\epsilon^T M \\epsilon) | X) = \\mathbb{E}(\\text{tr}(M \\epsilon \\epsilon^T) | X)$$\n",
    "\n",
    "Since $M$ is a function of $X$, we can take it out of the expectation:\n",
    "\n",
    "$$\\mathbb{E}(\\text{tr}(M \\epsilon \\epsilon^T) | X) = \\text{tr}(\\mathbb{E}(M \\epsilon \\epsilon^T | X))\n",
    "= \\text{tr}(M \\mathbb{E}(\\epsilon \\epsilon^T | X))\n",
    "= \\text{tr}(M \\sigma^2 I_1)\n",
    "= \\sigma^2 \\text{tr}(M)\n",
    "$$\n",
    "\n",
    "Finally, we can compute the trace of $M$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{tr}(M) &= \\text{tr}(I_n - X(X^T X)^{-1}X^T)\\\\\n",
    "&= \\text{tr}(I_n) - \\text{tr}(X(X^T X)^{-1}X^T) \\\\\n",
    "&= \\text{tr}(I_n) - \\text{tr}((X^T X)^{-1}X^T X) \\\\\n",
    "&= \\text{tr}(I_n) - \\text{tr}(I_k)  \\\\\n",
    "&= n - k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the unbiased estimator is\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{\\hat{\\epsilon}^T \\hat{\\epsilon}}{n - k}$$\n",
    "\n",
    "or, for our case where $k = 2$,\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\left( \\frac{1}{n - 2} \\right) \\sum_{i=1}^n \\hat{\\epsilon}_i^2$$\n",
    "\n",
    "Reference:  Greene, William H. Econometric analysis. Pearson Education India, 2003.  Chapter 4, pages 61-62."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.2**.  Prove the formulas for the standard errors in Theorem 14.8.  You should regard the $X_i$'s as fixed constants.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\beta} | X^n) = \\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{V}(\\hat{\\beta} | X^n) = \\frac{\\sigma^2}{n s_X^2} \\begin{pmatrix} \n",
    "\\frac{1}{n} \\sum_{i=1}^n X_i^2 & -\\overline{X}_n \\\\\n",
    "-\\overline{X}_n & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$s_X^2 = n^{-1} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2$$\n",
    "\n",
    "$$\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_0) = \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\sqrt{\\frac{\\sum_{i=1}^n X_i^2}{n}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  \n",
    "\n",
    "The formulas follow immediately by performing the suggested replacement on the diagonal elements of $\\mathbb{V}(\\hat{\\beta} | X^n)$ from Theorem 14.8. From the diagonals, replacing $\\hat{\\sigma}$ for $\\sigma$:\n",
    "\n",
    "$$\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_0)^2 = \\frac{\\hat{\\sigma}^2}{n s_X^2} \\frac{\\sum_{i=1}^n X_i^2}{n}\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_1)^2 = \\frac{\\hat{\\sigma}^2}{n s_X^2} \\cdot 1$$\n",
    "\n",
    "Results follow by taking the square root.\n",
    "\n",
    "We will also prove the variance matrix result itself, following from the notation and proof used on exercise 14.9.1 (again, adapting from Greene):\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1}X^T y = (X^T X)^{-1}X^T (X \\beta + \\epsilon) = \\beta + (X^T X)^{-1}X^T \\epsilon\n",
    "$$\n",
    "\n",
    "Taking the variance conditional on $X$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{V}(\\hat{\\beta} | X) &= \\mathbb{V}(\\hat{\\beta} - \\beta | X) \\\\\n",
    "&= \\mathbb{E}((\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T | X)  \\\\\n",
    "&= \\mathbb{E}((X^T X)^{-1}X^T \\epsilon\\epsilon^T X(X^T X)^{-1} | X) \\\\\n",
    "&= (X^T X)^{-1}X^T \\mathbb{E}(\\epsilon\\epsilon^T | X) X(X^T X)^{-1} \\\\\n",
    "&= (X^T X)^{-1}X^T \\sigma^2 I X(X^T X)^{-1} \\\\\n",
    "&= \\sigma^2 (X^T X)^{-1} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But we have:\n",
    "\n",
    "$$ X^T X = \\begin{pmatrix}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "X_1 & X_2 & \\cdots & X_n\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "1 & X_1 \\\\\n",
    "1 & X_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & X_n\n",
    "\\end{pmatrix} = \n",
    "n \\begin{pmatrix}\n",
    "1 & \\overline{X}_n \\\\\n",
    "\\overline{X}_n & \\frac{1}{n}\\sum_{i=1}^n X_i^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "so we can verify its inverse is\n",
    "\n",
    "$$(X^T X)^{-1} = \\frac{1}{n s_X^2} \\begin{pmatrix}\n",
    "\\frac{1}{n} \\sum_{i=1}^n X_i^2 & - \\overline{X}_n \\\\\n",
    "-\\overline{X}_n & 1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "and so the result follows.\n",
    "\n",
    "Reference:  Greene, William H. Econometric analysis. Pearson Education India, 2003.  Chapter 4, page 59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.3**.  Consider the **regression through the origin** model:\n",
    "\n",
    "$$Y_i = \\beta X_i + \\epsilon$$\n",
    "\n",
    "Find the least squares estimate for $\\beta$.  Find the standard error of the estimate.  Find conditions that guarantee that the estimate is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Once more adopting notation from the solution of 14.9.1,  let\n",
    "\n",
    "$$ y = \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix},\n",
    "\\quad\n",
    "X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{pmatrix}$$\n",
    "\n",
    "and $\\beta$ is a scalar (or a $1 \\times 1$ matrix).\n",
    "\n",
    "The least squares estimate is, again,\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "which simplifies in this one-dimensional case to:\n",
    "\n",
    "$$\\hat{\\beta} = \\frac{\\sum_{i=1}^n X_i Y_i}{\\sum_{i=1}^n X_i^2}$$\n",
    "\n",
    "The unbiased estimator for $\\sigma^2$ is, with $k = 1$,\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n - 1} \\sum_{i=1}^n \\hat{\\epsilon}_i^2$$\n",
    "\n",
    "and the variance of $\\hat{\\beta}$ conditional of $X$ is:\n",
    "\n",
    "$$\\mathbb{V}(\\hat{\\beta} | X) = \\sigma^2 (X^T X)^{-1} = \\frac{\\sigma^2}{\\sum_{i=1}^n X_i^2}$$\n",
    "\n",
    "so the standard error of the estimate is\n",
    "\n",
    "$$\\hat{\\text{se}}(\\hat{\\beta}) = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n X_i^2}}$$\n",
    "\n",
    "These, of course, make the assumption that $X^T X$ is invertible -- that is, that the sum of squares of the $X_i$ variables is greater than 0.  This is only not the case when all covariates are 0, in which case the value of our estimator $\\hat{\\beta}$ would be irrelevant to determining the prediction outcome -- the system would be undetermined.\n",
    "\n",
    "Finally, note that $\\hat{\\beta}$ is also the MLE in the parameter space for regression through the origin.  As each measurement error $\\epsilon_i = Y_i - \\beta X_i$ is drawn from a normal distribution $N(0, \\sigma^2)$, the log-likelihood for a given parameter $\\beta$ is\n",
    "\n",
    "$$\\ell_n(\\beta) = -\\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (Y_i - \\beta X_i)^2 + C$$\n",
    "\n",
    "and so maximizing it is equivalent to minimizing $\\sum_{i=1}^n (Y_i - \\beta X_i)^2$, which is what the least squares procedure does.\n",
    "\n",
    "Since the MLE is consistent, the least squares estimate is also consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.4**.  Prove equation (14.24).\n",
    "\n",
    "$$ \\text{bias}(\\hat{R}_\\text{tr}(S)) = \\mathbb{E}(\\hat{R}_\\text{tr}(S)) - R(S) = -2 \\sum_i \\text{Cov}(\\hat{Y}_i, Y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The bias is:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathbb{E}(\\hat{R}_\\text{tr}(S)) - R(S) &= \\mathbb{E}\\left[\\sum_i (\\hat{Y}_i- Y_i)^2 \\right] - \\mathbb{E} \\left[ \\sum_i (\\hat{Y}_i - Y_i^*)^2 \\right] \\\\\n",
    "&= \\sum_i \\left( \\mathbb{E} \\left[\\hat{Y}_i^2 \\right] - 2 \\mathbb{E} \\left[\\hat{Y}_i Y_i \\right] + \\mathbb{E}\\left[Y_i^2 \\right] - \\mathbb{E} \\left[\\hat{Y}_i^2 \\right] + 2 \\mathbb{E} \\left[\\hat{Y}_i Y_i^* \\right] - \\mathbb{E}\\left[{Y_i^*}^2 \\right] \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The random variables $Y_i = \\beta X_i + \\epsilon_\\text{train}$ and $Y_i^* = \\beta X_i + \\epsilon_\\text{pred}$ are independent, as the errors during training and predition are independent, and the covariates $X_i$ and true parameter $\\beta$ are constant.  Since they are independent, $\\mathbb{E}[Y_i Y_i^*] = \\mathbb{E}[Y_i] \\mathbb{E}[Y_i^*]$.\n",
    "\n",
    "We also have $\\mathbb{E} [ {Y_i^*}^2 ] = \\mathbb{V}[Y_i^*] + \\mathbb{E}[Y_i^*]^2 = \\mathbb{V}[\\epsilon_\\text{pred}] + \\mathbb{E}[X \\beta]^2 = \\mathbb{V}[\\epsilon_\\text{train}] + \\mathbb{E}[Y_i]^2 = \\mathbb{E} [Y_i^2]$.\n",
    "\n",
    "Replacing both of those in the expression of bias above, we get the result:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(\\hat{R}_\\text{tr}(S)) - R(S) &= \n",
    "-2 \\sum_i \\left( \\mathbb{E}[Y_i \\hat{Y}_i] - \\mathbb{E}[Y_i] \\mathbb{E}[\\hat{Y}_i] \\right) \\\\\n",
    "&= -2 \\sum_i \\text{Cov}(\\hat{Y}_i, Y_i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.5**.  In the simple linear regression model, construct a Wald test for $H_0 : \\beta_1 = 17 \\beta_0$ versus $H_1 : \\beta_1 \\neq 17 \\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Let $\\delta = \\beta_1 - 17 \\beta_0$.  The MLE is $\\hat{\\delta} = \\hat{\\beta}_1 - 17 \\hat{\\beta}_0$, with estimated standard error $\\hat{\\text{se}}(\\hat{\\delta})$, where\n",
    "\n",
    "$$\\hat{\\text{se}}(\\hat{\\delta})^2 = \\hat{\\text{se}}(\\hat{\\beta}_1 - 17 \\hat{\\beta}_0)^2 = \\hat{\\text{se}}(\\hat{\\beta}_1)^2 + 17^2 \\hat{\\text{se}}(\\hat{\\beta}_0)^2 $$\n",
    "\n",
    "and the estimates for the parameter standard deviations are\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_0) = \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\sqrt{\\frac{\\sum_{i=1}^n X_i^2}{n}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "The Wald test then checks if $|W| < z_{\\alpha / 2}$, where \n",
    "\n",
    "$$W = \\frac{\\hat{\\delta} - 0}{\\hat{\\text{se}}(\\hat{\\delta})} \n",
    "= \\frac{\\hat{\\beta}_1 - 17 \\hat{\\beta}_0}{\\sqrt{\\hat{\\text{se}}(\\hat{\\beta}_1)^2 + 17^2 \\hat{\\text{se}}(\\hat{\\beta}_0)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.6**.  Get the passenger car mileage data from \n",
    "\n",
    "http://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html\n",
    "\n",
    "**(a)** Fit a simple linear regression model to predict MPG (miles per gallon) from HP (horsepower).  Summarize your analysis including a plot of the data with the fitted line.\n",
    "\n",
    "**(b)** Repeat the analysis but use log(MPG) as the response.  Compare the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "We would ordinarily use a library to do linear regression, but given this chapter is specifically on linear regression formulas, let's do all of the calculations on matrix algebra \"by hand\" instead -- and compare the results with statsmodels OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Provided link is dead.  Data was found elsewhere online.\n",
    "data = pd.read_csv('data/carmileage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression(X, Y):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Create new column with all 1s for intercept at start\n",
    "    X.insert(0, 'const', 1)\n",
    "    \n",
    "    # Least squares solution\n",
    "    beta_hat = (np.linalg.inv(X.T @ X) @ X.T @ Y).to_numpy()\n",
    "\n",
    "    # Predicted solutions\n",
    "    Y_pred = X @ beta_hat\n",
    "\n",
    "    # Prediction errors\n",
    "    epsilon_hat = Y_pred - Y\n",
    "\n",
    "    # Error on training data\n",
    "    training_error = epsilon_hat.T @ epsilon_hat\n",
    "    \n",
    "    # Estimated error variance\n",
    "    sigma2_hat = (training_error / (Y.shape[0] - X.shape[1]))\n",
    "\n",
    "    # Parameter estimated standard errors\n",
    "    se_beta_hat = np.sqrt(sigma2_hat * np.diag(np.linalg.inv(X.T @ X))).T\n",
    "\n",
    "    # t statistic for estimated parameters being non-zero\n",
    "    t_values = beta_hat.reshape(-1) / se_beta_hat\n",
    "\n",
    "    # p-values for estimated parameters being non-zero\n",
    "    p_values = 2 * (1 - t.cdf(np.abs(t_values), X.shape[0] - 1))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'coef': beta_hat.reshape(-1),\n",
    "        'std err': se_beta_hat.reshape(-1),\n",
    "        't': t_values.reshape(-1),\n",
    "        'P > |t|': p_values.reshape(-1) \n",
    "        }, index=X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['MPG']\n",
    "X = data[['HP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P &gt; |t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>50.066078</td>\n",
       "      <td>1.569487</td>\n",
       "      <td>31.899650</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP</th>\n",
       "      <td>-0.139023</td>\n",
       "      <td>0.012069</td>\n",
       "      <td>-11.519295</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            coef   std err          t  P > |t|\n",
       "const  50.066078  1.569487  31.899650      0.0\n",
       "HP     -0.139023  0.012069 -11.519295      0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using manually coded solution\n",
    "get_regression(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    MPG   R-squared:                       0.624\n",
      "Model:                            OLS   Adj. R-squared:                  0.619\n",
      "Method:                 Least Squares   F-statistic:                     132.7\n",
      "Date:                Fri, 23 Apr 2021   Prob (F-statistic):           1.15e-18\n",
      "Time:                        20:07:54   Log-Likelihood:                -264.61\n",
      "No. Observations:                  82   AIC:                             533.2\n",
      "Df Residuals:                      80   BIC:                             538.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         50.0661      1.569     31.900      0.000      46.943      53.189\n",
      "HP            -0.1390      0.012    -11.519      0.000      -0.163      -0.115\n",
      "==============================================================================\n",
      "Omnibus:                       22.759   Durbin-Watson:                   0.721\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               31.329\n",
      "Skew:                           1.246   Prob(JB):                     1.57e-07\n",
      "Kurtosis:                       4.722   Cond. No.                         299.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Using statsmodels\n",
    "results = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_fit(results, 1, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exactly the same exercise, but fit on a different response variable\n",
    "\n",
    "Y = np.log(data['MPG']).rename('log MPG')\n",
    "X = data[['HP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P &gt; |t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>4.013229</td>\n",
       "      <td>0.040124</td>\n",
       "      <td>100.021194</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP</th>\n",
       "      <td>-0.004589</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>-14.873129</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           coef   std err           t  P > |t|\n",
       "const  4.013229  0.040124  100.021194      0.0\n",
       "HP    -0.004589  0.000309  -14.873129      0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using manually coded solution\n",
    "get_regression(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                log MPG   R-squared:                       0.734\n",
      "Model:                            OLS   Adj. R-squared:                  0.731\n",
      "Method:                 Least Squares   F-statistic:                     221.2\n",
      "Date:                Fri, 23 Apr 2021   Prob (F-statistic):           9.62e-25\n",
      "Time:                        20:08:24   Log-Likelihood:                 36.047\n",
      "No. Observations:                  82   AIC:                            -68.09\n",
      "Df Residuals:                      80   BIC:                            -63.28\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.0132      0.040    100.021      0.000       3.933       4.093\n",
      "HP            -0.0046      0.000    -14.873      0.000      -0.005      -0.004\n",
      "==============================================================================\n",
      "Omnibus:                        4.454   Durbin-Watson:                   1.026\n",
      "Prob(Omnibus):                  0.108   Jarque-Bera (JB):                3.827\n",
      "Skew:                           0.516   Prob(JB):                        0.148\n",
      "Kurtosis:                       3.236   Cond. No.                         299.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Using statsmodels\n",
    "results = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_fit(results, 1, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.7**.  Get the passenger car mileage data from \n",
    "\n",
    "http://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html\n",
    "\n",
    "**(a)** Fit a multiple linear regression model to predict MPG (miles per gallon) from HP (horsepower).  Summarize your analysis including a plot of the data with the fitted line.\n",
    "\n",
    "**(b)** Use Mallow's $C_p$ to select a best sub-model.  To search through all models try (i) all possible models, (ii) forward stepwise, (iii) backward stepwise.  Summarize your findings.\n",
    "\n",
    "**(c)** Repeat (b) but use BIC.  Compare the results.\n",
    "\n",
    "**(d)** Now use Lasso and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The exercise wording is unclear -- if we specify that HP is the only covariate, the multiple linear regression and the simple linear regression are the same, and (a) is like the previous exercise.\n",
    "\n",
    "We will elect to use VOL, HP, SP and WT as the covariates, and MPG as the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full model\n",
    "\n",
    "features = ['HP', 'SP', 'WT', 'VOL']\n",
    "\n",
    "Y = data['MPG']\n",
    "X = data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P &gt; |t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>192.437753</td>\n",
       "      <td>23.531613</td>\n",
       "      <td>8.177839</td>\n",
       "      <td>3.351097e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP</th>\n",
       "      <td>0.392212</td>\n",
       "      <td>0.081412</td>\n",
       "      <td>4.817602</td>\n",
       "      <td>6.671547e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP</th>\n",
       "      <td>-1.294818</td>\n",
       "      <td>0.244773</td>\n",
       "      <td>-5.289864</td>\n",
       "      <td>1.019119e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WT</th>\n",
       "      <td>-1.859804</td>\n",
       "      <td>0.213363</td>\n",
       "      <td>-8.716617</td>\n",
       "      <td>2.888800e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOL</th>\n",
       "      <td>-0.015645</td>\n",
       "      <td>0.022825</td>\n",
       "      <td>-0.685425</td>\n",
       "      <td>4.950327e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef    std err         t       P > |t|\n",
       "const  192.437753  23.531613  8.177839  3.351097e-12\n",
       "HP       0.392212   0.081412  4.817602  6.671547e-06\n",
       "SP      -1.294818   0.244773 -5.289864  1.019119e-06\n",
       "WT      -1.859804   0.213363 -8.716617  2.888800e-13\n",
       "VOL     -0.015645   0.022825 -0.685425  4.950327e-01"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using manually coded solution\n",
    "get_regression(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    MPG   R-squared:                       0.873\n",
      "Model:                            OLS   Adj. R-squared:                  0.867\n",
      "Method:                 Least Squares   F-statistic:                     132.7\n",
      "Date:                Fri, 23 Apr 2021   Prob (F-statistic):           9.98e-34\n",
      "Time:                        20:09:04   Log-Likelihood:                -220.00\n",
      "No. Observations:                  82   AIC:                             450.0\n",
      "Df Residuals:                      77   BIC:                             462.0\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        192.4378     23.532      8.178      0.000     145.580     239.295\n",
      "HP             0.3922      0.081      4.818      0.000       0.230       0.554\n",
      "SP            -1.2948      0.245     -5.290      0.000      -1.782      -0.807\n",
      "WT            -1.8598      0.213     -8.717      0.000      -2.285      -1.435\n",
      "VOL           -0.0156      0.023     -0.685      0.495      -0.061       0.030\n",
      "==============================================================================\n",
      "Omnibus:                       14.205   Durbin-Watson:                   1.148\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.605\n",
      "Skew:                           0.784   Prob(JB):                     9.12e-05\n",
      "Kurtosis:                       4.729   Cond. No.                     1.16e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Using statsmodels\n",
    "results = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_fit(results, 1, ax=ax)\n",
    "ax.set_ylabel(\"MPG\")\n",
    "ax.set_xlabel(\"HP\")\n",
    "ax.set_title(\"Linear Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "First, let's create helper functions to calculate the model variance and Mallow's $C_p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_variance(X, Y):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Create new column with all 1s for intercept at start\n",
    "    X.insert(0, 'const', 1)\n",
    "    \n",
    "    # Least squares solution\n",
    "    beta_hat = (np.linalg.inv(X.T @ X) @ X.T @ Y).to_numpy()\n",
    "\n",
    "    # Predicted solutions\n",
    "    Y_pred = X @ beta_hat\n",
    "\n",
    "    # Prediction errors\n",
    "    epsilon_hat = Y_pred - Y\n",
    "\n",
    "    # Error on training data\n",
    "    training_error = epsilon_hat.T @ epsilon_hat\n",
    "    \n",
    "    # Estimated error variance\n",
    "    return (training_error / (Y.shape[0] - X.shape[1]))\n",
    "    \n",
    "\n",
    "def get_mallow_cp(X, Y, S, full_model_variance):\n",
    "    if len(S) > 0:\n",
    "        X = X[list(S)].copy()\n",
    "        # Create new column with all 1s for intercept at start\n",
    "        X.insert(0, 'const', 1)\n",
    "    else:\n",
    "        X = pd.DataFrame({'const': np.ones_like(Y)})\n",
    "    \n",
    "    # Least squares solution\n",
    "    beta_hat = (np.linalg.inv(X.T @ X) @ X.T @ Y).to_numpy()\n",
    "\n",
    "    # Predicted solutions\n",
    "    Y_pred = X @ beta_hat\n",
    "\n",
    "    # Prediction errors\n",
    "    epsilon_hat = Y_pred - Y\n",
    "\n",
    "    # Error on training data\n",
    "    partial_training_error = epsilon_hat.T @ epsilon_hat\n",
    "    \n",
    "    # Increase size of S by to account for constant covariate\n",
    "    return partial_training_error + 2 * (len(S) + 1) * full_model_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculate and save the full model variance **once**, sine it's used for every candidate model -- and use it to define our custom score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_variance = get_model_variance(X, Y)\n",
    "\n",
    "def score_mallow_cp(S):\n",
    "    return get_mallow_cp(X, Y, S, full_model_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's do the submodel search.\n",
    "\n",
    "First approach is an explicit search through all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipe from itertools documentation, https://docs.python.org/2.7/library/itertools.html#recipes\n",
    "\n",
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the powerset and calculate the score for each value\n",
    "results = [(S, score_mallow_cp(S)) for S in powerset(features)]\n",
    "    \n",
    "# Format as dataframe for ease of presentation\n",
    "results = pd.DataFrame(results, columns=['S', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>()</td>\n",
       "      <td>8134.147794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(HP,)</td>\n",
       "      <td>3102.805578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(SP,)</td>\n",
       "      <td>4318.234609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(WT,)</td>\n",
       "      <td>1519.372094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(VOL,)</td>\n",
       "      <td>7059.223052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(HP, SP)</td>\n",
       "      <td>2436.578842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(HP, WT)</td>\n",
       "      <td>1510.677511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(HP, VOL)</td>\n",
       "      <td>2354.823180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(SP, WT)</td>\n",
       "      <td>1463.076513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(SP, VOL)</td>\n",
       "      <td>3056.598857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(WT, VOL)</td>\n",
       "      <td>1542.174631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(HP, SP, WT)</td>\n",
       "      <td>1140.390871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(HP, SP, VOL)</td>\n",
       "      <td>2147.886521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(HP, WT, VOL)</td>\n",
       "      <td>1507.484328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(SP, WT, VOL)</td>\n",
       "      <td>1443.795004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(HP, SP, WT, VOL)</td>\n",
       "      <td>1160.807643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    S        score\n",
       "0                  ()  8134.147794\n",
       "1               (HP,)  3102.805578\n",
       "2               (SP,)  4318.234609\n",
       "3               (WT,)  1519.372094\n",
       "4              (VOL,)  7059.223052\n",
       "5            (HP, SP)  2436.578842\n",
       "6            (HP, WT)  1510.677511\n",
       "7           (HP, VOL)  2354.823180\n",
       "8            (SP, WT)  1463.076513\n",
       "9           (SP, VOL)  3056.598857\n",
       "10          (WT, VOL)  1542.174631\n",
       "11       (HP, SP, WT)  1140.390871\n",
       "12      (HP, SP, VOL)  2147.886521\n",
       "13      (HP, WT, VOL)  1507.484328\n",
       "14      (SP, WT, VOL)  1443.795004\n",
       "15  (HP, SP, WT, VOL)  1160.807643"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(HP, SP, WT)</td>\n",
       "      <td>1140.390871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               S        score\n",
       "11  (HP, SP, WT)  1140.390871"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.score == results.score.min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach recommends the features HP, SP, and WT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's do forward stepwise feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(WT, SP, HP)</td>\n",
       "      <td>1140.390871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              S        score\n",
       "0  (WT, SP, HP)  1140.390871"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_subset = []\n",
    "current_score = score_mallow_cp(current_subset)\n",
    "\n",
    "while len(current_subset) < len(features):\n",
    "    best_score, best_subset = current_score, current_subset\n",
    "    updated = False\n",
    "    for f in features:\n",
    "        if f not in current_subset:\n",
    "            candidate_subset = current_subset + [f]\n",
    "            candidate_score = score_mallow_cp(candidate_subset)\n",
    "            if candidate_score < best_score:\n",
    "                best_score, best_subset = candidate_score, candidate_subset\n",
    "                updated = True              \n",
    "    if not updated:\n",
    "        break\n",
    "        \n",
    "    current_score, current_subset = best_score, best_subset\n",
    "    \n",
    "pd.DataFrame([[tuple(current_subset), current_score]], columns=['S', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach also recommends selecting these 3 features (order is irrelevant): WT, SP, HP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's so backward stepwise feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(HP, SP, WT)</td>\n",
       "      <td>1140.390871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              S        score\n",
       "0  (HP, SP, WT)  1140.390871"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_subset = features\n",
    "current_score = score_mallow_cp(current_subset)\n",
    "\n",
    "while len(current_subset) > 0:\n",
    "    best_score, best_subset = current_score, current_subset\n",
    "    updated = False\n",
    "    for f in features:\n",
    "        if f in current_subset:\n",
    "            candidate_subset = [a for a in current_subset if a != f]\n",
    "            candidate_score = score_mallow_cp(candidate_subset)\n",
    "            if candidate_score < best_score:\n",
    "                best_score, best_subset = candidate_score, candidate_subset\n",
    "                updated = True              \n",
    "    if not updated:\n",
    "        break\n",
    "        \n",
    "    current_score, current_subset = best_score, best_subset\n",
    "    \n",
    "pd.DataFrame([[tuple(current_subset), current_score]], columns=['S', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach also recommends selecting the same 3 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  This is analogous to (b), using the new scoring function.\n",
    "\n",
    "Note that the book's definition of BIC is:\n",
    "\n",
    "$$ \\text{BIC}(S) = \\text{RSS}(S) + 2 |S| \\hat{\\sigma}^2 $$\n",
    "\n",
    "while other sources apply a log to this quantity and scale it by the number of observations $n$, e.g.\n",
    "\n",
    "$$ \\text{BIC} = n \\log \\left( \\text{RSS} / n \\right) + |S| \\log n $$\n",
    "\n",
    "We will use the later definition for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bic(X, Y, S):\n",
    "    if len(S) > 0:\n",
    "        X = X[list(S)].copy()\n",
    "        # Create new column with all 1s for intercept at start\n",
    "        X.insert(0, 'const', 1)\n",
    "    else:\n",
    "        X = pd.DataFrame({'const': np.ones_like(Y)})\n",
    "    \n",
    "    # Least squares solution\n",
    "    beta_hat = (np.linalg.inv(X.T @ X) @ X.T @ Y).to_numpy()\n",
    "\n",
    "    # Predicted solutions\n",
    "    Y_pred = X @ beta_hat\n",
    "\n",
    "    # Prediction errors\n",
    "    epsilon_hat = Y_pred - Y\n",
    "\n",
    "    # Error on training data\n",
    "    rss = epsilon_hat.T @ epsilon_hat\n",
    "    \n",
    "    n = Y.shape[0]\n",
    "    k = X.shape[1]\n",
    "    \n",
    "    return n * np.log(rss / n) + k * np.log(n)\n",
    "\n",
    "def score_bic(S):\n",
    "    return get_bic(X, Y, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the powerset and calculate the score for each value\n",
    "results = [(S, score_bic(S)) for S in powerset(features)]\n",
    "    \n",
    "# Format as dataframe for ease of presentation\n",
    "results = pd.DataFrame(results, columns=['S', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>()</td>\n",
       "      <td>381.100039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(HP,)</td>\n",
       "      <td>305.324815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(SP,)</td>\n",
       "      <td>332.832040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(WT,)</td>\n",
       "      <td>245.266568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(VOL,)</td>\n",
       "      <td>373.531556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(HP, SP)</td>\n",
       "      <td>288.594470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(HP, WT)</td>\n",
       "      <td>247.670065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(HP, VOL)</td>\n",
       "      <td>285.699095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(SP, WT)</td>\n",
       "      <td>244.895261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(SP, VOL)</td>\n",
       "      <td>307.747647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(WT, VOL)</td>\n",
       "      <td>249.455822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(HP, SP, WT)</td>\n",
       "      <td>225.425717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(HP, SP, VOL)</td>\n",
       "      <td>281.219751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(HP, WT, VOL)</td>\n",
       "      <td>250.346085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(SP, WT, VOL)</td>\n",
       "      <td>246.530268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(HP, SP, WT, VOL)</td>\n",
       "      <td>229.333642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    S       score\n",
       "0                  ()  381.100039\n",
       "1               (HP,)  305.324815\n",
       "2               (SP,)  332.832040\n",
       "3               (WT,)  245.266568\n",
       "4              (VOL,)  373.531556\n",
       "5            (HP, SP)  288.594470\n",
       "6            (HP, WT)  247.670065\n",
       "7           (HP, VOL)  285.699095\n",
       "8            (SP, WT)  244.895261\n",
       "9           (SP, VOL)  307.747647\n",
       "10          (WT, VOL)  249.455822\n",
       "11       (HP, SP, WT)  225.425717\n",
       "12      (HP, SP, VOL)  281.219751\n",
       "13      (HP, WT, VOL)  250.346085\n",
       "14      (SP, WT, VOL)  246.530268\n",
       "15  (HP, SP, WT, VOL)  229.333642"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(HP, SP, WT)</td>\n",
       "      <td>225.425717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               S       score\n",
       "11  (HP, SP, WT)  225.425717"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.score == results.score.min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward stepwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(WT, SP, HP)</td>\n",
       "      <td>225.425717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              S       score\n",
       "0  (WT, SP, HP)  225.425717"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_subset = []\n",
    "current_score = score_bic(current_subset)\n",
    "\n",
    "while len(current_subset) < len(features):\n",
    "    best_score, best_subset = current_score, current_subset\n",
    "    updated = False\n",
    "    for f in features:\n",
    "        if f not in current_subset:\n",
    "            candidate_subset = current_subset + [f]\n",
    "            candidate_score = score_bic(candidate_subset)\n",
    "            if candidate_score < best_score:\n",
    "                best_score, best_subset = candidate_score, candidate_subset\n",
    "                updated = True              \n",
    "    if not updated:\n",
    "        break\n",
    "        \n",
    "    current_score, current_subset = best_score, best_subset\n",
    "    \n",
    "pd.DataFrame([[tuple(current_subset), current_score]], columns=['S', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward stepwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(HP, SP, WT)</td>\n",
       "      <td>225.425717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              S       score\n",
       "0  (HP, SP, WT)  225.425717"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_subset = features\n",
    "current_score = score_bic(current_subset)\n",
    "\n",
    "while len(current_subset) > 0:\n",
    "    best_score, best_subset = current_score, current_subset\n",
    "    updated = False\n",
    "    for f in features:\n",
    "        if f in current_subset:\n",
    "            candidate_subset = [a for a in current_subset if a != f]\n",
    "            candidate_score = score_bic(candidate_subset)\n",
    "            if candidate_score < best_score:\n",
    "                best_score, best_subset = candidate_score, candidate_subset\n",
    "                updated = True              \n",
    "    if not updated:\n",
    "        break\n",
    "        \n",
    "    current_score, current_subset = best_score, best_subset\n",
    "    \n",
    "pd.DataFrame([[tuple(current_subset), current_score]], columns=['S', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All approaches recommend the same feature selection as the previous method -- select the 3 features HP, SP, and WT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  To use Lasso, we will need to minimize the L1 loss function for an arbitrary penalty parameter $\\lambda$:\n",
    "\n",
    "$$ \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^k | \\beta_j | $$\n",
    "\n",
    "Since we are including a penalty parameter that affects both estimation and model selection, we will need to calculate this loss function on some test data distinct from the training data -- the recommended approach being to use leave-one-out cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Lasso loss function\n",
    "def lasso_loss(Y, Y_pred, beta, l1_penalty):\n",
    "    error = Y - Y_pred\n",
    "    return error.T @ error + l1_penalty * sum(abs(beta))\n",
    "\n",
    "# Regularized fit\n",
    "def fit_regularized(X, Y, l1_penalty):\n",
    "    def loss_function(beta):\n",
    "        return lasso_loss(Y, X @ beta, beta, l1_penalty)\n",
    "    \n",
    "    # Use the solution without penalties as an initial guess\n",
    "    beta_initial_guess = (np.linalg.inv(X.T @ X) @ X.T @ Y).to_numpy()\n",
    "    return minimize(loss_function, beta_initial_guess, method = 'Powell',\n",
    "                    options={'xtol': 1e-8, 'disp': False, 'maxiter': 10000 }) \n",
    "\n",
    "# Leave-one-out cross-validation\n",
    "def leave_one_out_cv_risk(X, Y, fitting_function):\n",
    "    n = X.shape[1]\n",
    "    total_risk = 0\n",
    "    for i in range(n):\n",
    "        XX = pd.concat([X.iloc[:i], X.iloc[i + 1:]])\n",
    "        YY = pd.concat([Y.iloc[:i], Y.iloc[i + 1:]])\n",
    "        beta = fitting_function(XX, YY).x\n",
    "        validation_error = Y.iloc[i] - X.iloc[i] @ beta\n",
    "        total_risk += validation_error * validation_error\n",
    "    return total_risk / n\n",
    "\n",
    "# Optimize over penalty parameter with best cross-validation risk\n",
    "def optimize_l1_penalty(X, Y):\n",
    "    def loss_function(l1_penalty_signed):\n",
    "        # Ensure l1_penalty >= 0\n",
    "        l1_penalty = abs(l1_penalty_signed)\n",
    "        return leave_one_out_cv_risk(X, Y, lambda xx, yy: fit_regularized(xx, yy, l1_penalty))\n",
    "    \n",
    "    l1_penalty_initial_guess = 0.0\n",
    "    return minimize(loss_function, l1_penalty_initial_guess, method = 'Powell',\n",
    "                   options={'xtol': 1e-8, 'disp': True, 'maxiter': 10000 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dimension with constants, so the regressions have an intercept\n",
    "X_c = X.copy()\n",
    "X_c.insert(0, 'const', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 61.554007\n",
      "         Iterations: 1\n",
      "         Function evaluations: 37\n",
      "Selected penalty:  3.1968611201740973e-06\n"
     ]
    }
   ],
   "source": [
    "# Optimize cross validation risk over penalties\n",
    "best_penalty_res = optimize_l1_penalty(X_c, Y)\n",
    "selected_l1_penalty = abs(best_penalty_res.x)\n",
    "\n",
    "print(\"Selected penalty: \", selected_l1_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>192.437753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP</th>\n",
       "      <td>0.392212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP</th>\n",
       "      <td>-1.294818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WT</th>\n",
       "      <td>-1.859804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOL</th>\n",
       "      <td>-0.015645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef\n",
       "const  192.437753\n",
       "HP       0.392212\n",
       "SP      -1.294818\n",
       "WT      -1.859804\n",
       "VOL     -0.015645"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-fit with selected penalty over the whole dataset\n",
    "selected_fit = fit_regularized(X_c, Y, selected_l1_penalty)\n",
    "beta = selected_fit.x\n",
    "pd.DataFrame(beta.reshape(-1, 1), index=X_c.columns, columns=['coef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best leave-one-out cross validation for the Lasso procedure is achieved (according to our optimizer) at $\\lambda \\approx$  0, where all covariants have a non-zero coefficient (i.e. $\\beta_i \\neq 0$ for all $i$).  \n",
    "\n",
    "In other words, Lasso with leave-one-out cross validation selects the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.8**.  Assume that the errors are Normal.  Show that the model with highest AIC is the model with lowest Mallows $C_p$ statistic.\n",
    "\n",
    "Mallows' $C_p$:\n",
    "\n",
    "$$\\hat{R}(S) = \\hat{R}_\\text{tr}(S) + 2 |S| \\hat{\\sigma}^2$$\n",
    "\n",
    "AIC:\n",
    "\n",
    "$$  \\text{AIC} = \\ell_S - |S|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "For the given definition, note that $-2 \\text{AIC} \\hat{\\sigma}^2 = -2 \\ell_S \\hat{\\sigma}^2 + 2|S| \\hat{\\sigma}^2 = \\hat{R}_\\text{tr}(S) + 2 |S| \\hat{\\sigma}^2 = \\hat{R}(S)$, given the log likelihood of a Normal model -- so maximizing AIC is equivalent to minimizing Mallows $C_p$.\n",
    "\n",
    "Note that a more general result holds: rather than assuming the errors are Normal, one can assume that the distributions are part of a spherically symmetric family, i.e., modifying the distributions under an orthogonal transformation (and potentially removing invariance).  See: Boisbunon, Aurélie, et al. \"AIC, Cp and estimators of loss for elliptically symmetric distributions.\" arXiv preprint arXiv:1308.2766 (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14.9.9**.  In this question we will take a closer look at the AIC method.  Let $X_1, \\dots, X_n$ be iid observations.  Consider two models $\\mathcal{M}_0$ and $\\mathcal{M}_1$.  Under $\\mathcal{M}_0$ the data are assumed to be $N(0, 1)$ while under $M_1$ the data are assumed to be $N(\\theta, 1)$ for some unknown $\\theta \\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{M}_0 : X_1, \\dots, X_n &\\sim N(0, 1) \\\\\n",
    "\\mathcal{M}_1 : X_1, \\dots, X_n &\\sim N(\\theta, 1), \\; \\theta \\in \\mathbb{R}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is just another way of viewing the hypothesis testing problem: $H_0: \\theta = 0$ versus $H_1: \\theta \\neq 0$.  Let $\\ell_n(\\theta)$ be the log-likelihood function.  The AIC score for a model is the log-likelihood at the MLE minus the number of parameters.  (Some people multiply this score by 2 but that is irrelevant).  Thus, the AIC score for $\\mathcal{M}_0$ is $\\text{AIC}_0 = \\ell_n(0)$ and the AIC score for $\\mathcal{M}_1$ is $\\text{AIC}_1 = \\ell_n(\\hat{\\theta}) - 1$.  Suppose we choose the model with highest AIC score.  Let $J_n$ denote the selected model:\n",
    "\n",
    "$$J_n = \\begin{cases}\n",
    "0 & \\text{if } \\text{AIC}_0 > \\text{AIC}_1 \\\\\n",
    "1 & \\text{if } \\text{AIC}_1 > \\text{AIC}_0\n",
    "\\end{cases}$$\n",
    "\n",
    "**(a)** Suppose that $\\mathcal{M}_0$ is the true model, i.e. $\\theta = 0$.  Find\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\mathbb{P}(J_n = 0) $$\n",
    "\n",
    "Now compute $\\lim_{n \\rightarrow \\infty} \\mathbb{P}(J_n = 0)$ when $\\theta \\neq 0$.\n",
    "\n",
    "**(b)** The fact that $\\lim_{n \\rightarrow \\infty} \\mathbb{P}(J_n = 0) \\neq 1$ when $\\theta = 0$ is why some people say that AIC \"overfits\". But this is not quite true as we shall now see.  Let $\\phi_\\theta(x)$ denote a Normal density function with mean $\\theta$ and variance 1.  Define\n",
    "\n",
    "$$ \\hat{f}_n(x) = \\begin{cases}\n",
    "\\phi_0(x) & \\text{if } J_n = 0 \\\\\n",
    "\\phi_\\overline{\\theta}(x) & \\text{if } J_n = 1\n",
    "\\end{cases}$$\n",
    "\n",
    "If $\\theta = 0$, show that $D(\\phi_0, \\hat{f}_n) \\xrightarrow{\\text{P}} 0$ as $n \\rightarrow \\infty$ where\n",
    "\n",
    "$$ D(f, g) = \\int f(x) \\log \\left( \\frac{f(x)}{g(x)} \\right) dx $$\n",
    "\n",
    "is the Kullback-Leibler distance.  Show that $D(\\phi_\\theta, \\hat{f}_n) \\xrightarrow{\\text{P}} 0$ if $\\theta \\neq 0$.  Hence, AIC consistently estimates the true density even if it \"overshoots\" the correct model.\n",
    "\n",
    "REMARK: If you are feeling ambitious, repeat the analysis for BIC which is the log-likelihood minus $(p / 2) \\log n$ where $p$ is the number of parameters and $n$ is the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "**(a)**  Note that the log-likelihood of the distribution $N(\\mu, \\sigma^2)$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell_n(\\mu, \\sigma^2) &= \\log \\prod_{i=1}^n f(X_i | \\mu, \\sigma^2) \\\\\n",
    "&= \\sum_{i=1}^n \\log f(X_i | \\mu, \\sigma^2) \\\\\n",
    "&= - \\frac{n}{2} \\log 2\\pi - \\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (X_i - \\mu)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(J_n = 0) &= \\mathbb{P}(\\text{AIC}_0 > \\text{AIC}_1) \\\\\n",
    "&= \\mathbb{P}(\\ell_n(0) > \\ell_n(\\hat{\\theta}) - 1) \\\\\n",
    "&= \\mathbb{P} \\left(-\\frac{n}{2} \\log 2\\pi - 0 - \\frac{1}{2} \\sum_{i=1}^n (X_i - 0)^2 > -\\frac{n}{2} \\log 2\\pi - 0 - \\frac{1}{2} \\sum_{i=1}^n (X_i - \\hat{\\theta})^2 - 1 \\right) \\\\\n",
    "&= \\mathbb{P}\\left( -\\frac{1}{2} \\sum_{i=1}^n X_i^2 > -\\frac{1}{2} \\sum_{i=1}^n (X_i - \\hat{\\theta})^2 - 1\\right) \\\\\n",
    "&= \\mathbb{P}\\left( \\sum_{i=1}^n \\left((X_i - \\hat{\\theta})^2 - X_i^2 \\right) > -2 \\right) \\\\\n",
    "&= \\mathbb{P}\\left( n \\hat{\\theta}^2 - 2 \\hat{\\theta} \\sum_{i=1}^n X_i > -2 \\right) \\\\\n",
    "&= \\mathbb{P}\\left( n \\overline{X}_n^2 - 2 \\overline{X}_n n \\overline{X}_n > -2\\right) \\\\\n",
    "&= \\mathbb{P}\\left( -n \\overline{X}_n^2 > -2 \\right) \\\\\n",
    "&= \\mathbb{P}\\left(-\\sqrt{\\frac{2}{n}} < \\overline{X}_n < \\sqrt{\\frac{2}{n}} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But $X_i \\sim N(\\theta, 1)$, so $n \\overline{X}_n = \\sum_i X_i \\sim N(n\\theta, n)$ and $\\overline{X}_n \\sim N(\\theta, 1/n)$.  Then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(J_n = 0) &= \\mathbb{P}\\left(-\\sqrt{\\frac{2}{n}} < \\overline{X}_n < \\sqrt{\\frac{2}{n}} \\right) \\\\\n",
    "&= \\mathbb{P}\\left(\\frac{-\\sqrt{\\frac{2}{n}} - \\theta}{\\sqrt{1/n}} < \\frac{\\overline{X}_n - \\theta}{\\sqrt{1/n}} < \\frac{\\sqrt{\\frac{2}{n}} - \\theta}{\\sqrt{1/n}} \\right) \\\\\n",
    "&= \\mathbb{P}\\left(-\\sqrt{2} - \\sqrt{n}\\theta < Z < \\sqrt{2} - \\sqrt{n}\\theta \\right) \\\\\n",
    "&= \\Phi(\\sqrt{2} - \\sqrt{n}\\theta) - \\Phi(-\\sqrt{2} - \\sqrt{n}\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\Phi$ is the CDF of the standard normal distribution $N(0, 1)$.\n",
    "\n",
    "When $\\theta = 0$,\n",
    "\n",
    "$$\\mathbb{P}(J_n = 0) = \\Phi(\\sqrt{2}) - \\Phi(-\\sqrt{2}) \\approx 0.8427 \\neq 0$$\n",
    "\n",
    "When $\\theta \\neq 0$,\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} \\mathbb{P}(J_n = 0) = \\lim_{n \\rightarrow \\infty}\\Phi(\\sqrt{2} - \\sqrt{n}\\theta) - \\Phi(-\\sqrt{2} - \\sqrt{n}\\theta) = \\lim_{n \\rightarrow \\infty}\\Phi(\\sqrt{n}\\theta) - \\lim_{n \\rightarrow \\infty}\\Phi(-\\sqrt{n}\\theta) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** We have:\n",
    "\n",
    "$$ D(\\phi_\\theta, \\hat{f}_n) = \\int \\phi_\\theta(x) \\log \\left(\\frac{\\phi_\\theta(x)}{\\hat{f}_n(x)} \\right) dx = \\int \\left[\\phi_\\theta(x) \\log \\phi_\\theta(x) - \\phi_\\theta(x) \\log \\hat{f}_n(x) \\right] dx $$\n",
    "\n",
    "But $\\lim_{n \\rightarrow \\infty} \\hat{f}_n(x) = \\phi_\\theta(x)$, so the integrand goes to 0 at each x, and so $D(\\phi_\\theta, \\hat{f}_n) \\xrightarrow{\\text{P}} 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK: I am feeling ambitious.**  Let $K_n$ denote the selected model:\n",
    "\n",
    "$$ K_n = \\begin{cases}\n",
    "0 & \\text{if } \\text{BIC}_0 > \\text{BIC}_1 \\\\\n",
    "1 & \\text{if } \\text{BIC}_1 > \\text{BIC}_0\n",
    "\\end{cases}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{BIC}_0 = \\ell_n(0) \\\\\n",
    "\\text{BIC}_1 = \\ell_n(\\hat{\\theta}) - \\frac{1}{2} \\log n\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(K_n = 0) &= \\mathbb{P}(\\text{BIC}_0 > \\text{BIC}_1) \\\\\n",
    "&= \\mathbb{P}\\left(\\ell_n(0) > \\ell_n(\\hat{\\theta}) - \\frac{1}{2} \\log n \\right) \\\\\n",
    "&= \\mathbb{P} \\left(-\\frac{n}{2} \\log 2\\pi - 0 - \\frac{1}{2} \\sum_{i=1}^n (X_i - 0)^2 > -\\frac{n}{2} \\log 2\\pi - 0 - \\frac{1}{2} \\sum_{i=1}^n (X_i - \\hat{\\theta})^2 - \\frac{1}{2} \\log n \\right) \\\\\n",
    "&= \\mathbb{P}\\left( -\\frac{1}{2} \\sum_{i=1}^n X_i^2 > -\\frac{1}{2} \\sum_{i=1}^n (X_i - \\hat{\\theta})^2 - \\frac{1}{2} \\log n\\right) \\\\\n",
    "&= \\mathbb{P}\\left( \\sum_{i=1}^n \\left((X_i - \\hat{\\theta})^2 - X_i^2 \\right) > -\\log n \\right) \\\\\n",
    "&= \\mathbb{P}\\left( n \\hat{\\theta}^2 - 2 \\hat{\\theta} \\sum_{i=1}^n X_i > -\\log n \\right) \\\\\n",
    "&= \\mathbb{P}\\left( n \\overline{X}_n^2 - 2 \\overline{X}_n n \\overline{X}_n > -\\log n\\right) \\\\\n",
    "&= \\mathbb{P}\\left( -n \\overline{X}_n^2 > -\\log n \\right) \\\\\n",
    "&= \\mathbb{P}\\left(-\\sqrt{\\frac{\\log n}{n}} < \\overline{X}_n < \\sqrt{\\frac{\\log n}{n}} \\right) \\\\\n",
    "&= \\mathbb{P}\\left(\\frac{-\\sqrt{\\frac{\\log n}{n}} - \\theta}{\\sqrt{1/n}} < \\frac{\\overline{X}_n - \\theta}{\\sqrt{1/n}} < \\frac{\\sqrt{\\frac{\\log n}{n}} - \\theta}{\\sqrt{1/n}} \\right) \\\\ \n",
    "&=  \\mathbb{P}\\left(-\\sqrt{\\log n} - \\sqrt{n}\\theta < Z < \\sqrt{\\log n} - \\sqrt{n}\\theta \\right) \\\\\n",
    "&= \\Phi(\\sqrt{\\log n} - \\sqrt{n}\\theta) - \\Phi(-\\sqrt{\\log n} - \\sqrt{n}\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As $O(\\sqrt{log n}) < O(\\sqrt{n})$, we get the result:\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\mathbb{P}(K_n = 0) = \\begin{cases}\n",
    "1 & \\text{if } \\theta = 0 \\\\\n",
    "0 & \\text{if } \\theta \\neq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Also, if we define:\n",
    "\n",
    "$$\\hat{g}_n(x) = \\begin{cases}\n",
    "\\phi_0(x) & \\text{if } K_n = 0 \\\\\n",
    "\\phi_\\overline{\\theta}(x) & \\text{if } K_n = 1\n",
    "\\end{cases}$$\n",
    "\n",
    "then, again, \n",
    "\n",
    "$$ D(\\phi_\\theta, \\hat{g}_n) = \\int \\phi_\\theta(x) \\log \\left(\\frac{\\phi_\\theta(x)}{\\hat{g}_n(x)} \\right) dx = \\int \\left[\\phi_\\theta(x) \\log \\phi_\\theta(x) - \\phi_\\theta(x) \\log \\hat{g}_n(x) \\right] dx $$\n",
    "\n",
    "But $\\lim_{n \\rightarrow \\infty} \\hat{g}_n(x) = \\phi_\\theta(x)$, so the integrand goes to 0 at each x, and so $D(\\phi_\\theta, \\hat{g}_n) \\xrightarrow{\\text{P}} 0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
