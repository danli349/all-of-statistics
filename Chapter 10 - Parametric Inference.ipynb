{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Parametric Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametric models** are of the form\n",
    "\n",
    "$$ \\mathfrak{F} = \\bigg\\{ f(x; \\theta) : \\; \\theta \\in \\Theta \\bigg\\} $$\n",
    "\n",
    "where $\\Theta \\subset \\mathbb{R}^k$ is the parameter space and $\\theta = (\\theta_1, \\dots, \\theta_k)$ is the parameter.  The problem of inference then reduces to the problem of estimating parameter $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Parameter of interest\n",
    "\n",
    "Often we are only interested in some function $T(\\theta)$.  For example, if $X \\sim N(\\mu, \\sigma^2)$ then the parameter is $\\theta = (\\mu, \\sigma)$.  If our goal is to estimate $\\mu$ then $\\mu = T(\\theta)$ is called the **parameter of interest** and $\\sigma$ is called  a **nuisance parameter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 The Method of Moments\n",
    "\n",
    "Suppose that the parameter $\\theta = (\\theta_1, \\dots, \\theta_n)$ has $k$ components.  For $1 \\leq j \\leq k$ define the $j$-th **moment**\n",
    "\n",
    "$$ \\alpha_j \\equiv \\alpha_j(\\theta) = \\mathbb{E}_\\theta(X^j) = \\int x^j dF_\\theta(x)$$\n",
    "\n",
    "and the $j$-th **sample moment**\n",
    "\n",
    "$$ \\hat{\\alpha}_j = \\frac{1}{n} \\sum_{i=1}^n X_i^j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **method of moments estimator** $\\hat{\\theta}_n$ is defined to be the value of $\\theta$ such that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_1(\\hat{\\theta}_n) &= \\hat{\\alpha_1} \\\\\n",
    "\\alpha_2(\\hat{\\theta}_n) &= \\hat{\\alpha_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_k(\\hat{\\theta}_n) &= \\hat{\\alpha_k}              \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This defines a system of $k$ equations with $k$ unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.6**.  Let $\\hat{\\theta}_n$ denote the method of moments estimator.  Under the conditions given in the appendix, the following statements hold:\n",
    "\n",
    "(1) The estimate $\\hat{\\theta}_n$ exists with probability tending to 1.\n",
    "\n",
    "(2) The estimate is consistent: $\\hat{\\theta}_n \\xrightarrow{\\text{P}} \\theta$.\n",
    "\n",
    "(3) The estimate is asymptotically Normal:\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\leadsto N(0, \\Sigma) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Sigma = g \\mathbb{E}_\\theta (Y Y^T) g^T \\\\\n",
    "Y = (X, X^2, \\dots, X^k)^T, \\quad g = (g_1, \\dots, g_k) \\quad \\text{and} \\quad g_j = \\partial \\alpha_j^{-1}(\\theta)/\\partial\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last statement in Theorem 10.6 can be used to find standard errors and confidence intervals.  However, there is an easier way: the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_1, \\dots, X_n$ be iid with PDF $f(x; \\theta)$.\n",
    "\n",
    "The **likelihood function** is defined by \n",
    "\n",
    "$$ \\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i; \\theta) $$\n",
    "\n",
    "The **log-likelihood function** is defined by $\\ell_n(\\theta) = \\log \\mathcal{L}_n(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function is just the joint density of the data, except we treat is as a function of parameter $\\theta$.  Thus $\\mathcal{L}_n : \\Theta \\rightarrow [0, \\infty)$.  The likelihood function is not a density function; in general it is not true that $\\mathcal{L}_n$ integrates to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **maximum likelihood estimator** MLE, denoted by $\\hat{\\theta}_n$, is the value of $\\theta$ that maximizes $\\mathcal{L}_n(\\theta)$.\n",
    "\n",
    "The maximum of $\\ell_n(\\theta)$ occurs at the same place as the maximum of $\\mathcal{L}_n(\\theta)$, so maximizing either leads to the same answer.  Often it's easier to maximize the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "### Figure 9.2\n",
    "\n",
    "n = 10\n",
    "x = runif(n)\n",
    "m = 100\n",
    "\n",
    "par(mfrow=c(2,2),pty=\"s\")\n",
    "f    = rep(0,m)\n",
    "grid = seq(0,2,length=m)\n",
    "f[grid < .75] = 1/.75\n",
    "plot(grid,f,type=\"l\",ylim=c(0,1.5),xlab=\"\",ylab=\"\",lwd=3,cex.lab=1.5)\n",
    "points(x,rep(.5,n))\n",
    "for(i in 1:n){\n",
    "     lines(c(x[i],x[i]),c(0,.5),lty=2,col=2)\n",
    "     }\n",
    "\n",
    "\n",
    "f    = rep(0,m)\n",
    "grid = seq(0,2,length=m)\n",
    "f[grid < 1] = 1\n",
    "plot(grid,f,type=\"l\",ylim=c(0,1.5),xlab=\"\",ylab=\"\",lwd=3,cex.lab=1.5)\n",
    "points(x,rep(.5,n))\n",
    "for(i in 1:n){\n",
    "     lines(c(x[i],x[i]),c(0,.5),lty=2,col=2)\n",
    "     }\n",
    "\n",
    "f    = rep(0,m)\n",
    "grid = seq(0,2,length=m)\n",
    "f[grid < 1.25] = 1/1.25\n",
    "plot(grid,f,type=\"l\",ylim=c(0,1.5),xlab=\"\",ylab=\"\",lwd=3,cex.lab=1.5)\n",
    "points(x,rep(.5,n))\n",
    "for(i in 1:n){\n",
    "     lines(c(x[i],x[i]),c(0,.5),lty=2,col=2)\n",
    "     }\n",
    "\n",
    "theta = seq(.6,1.5,length=m)     \n",
    "lik   = rep(0,m)\n",
    "lik   = 1/theta^n\n",
    "lik[theta < max(x)] =0\n",
    "plot(theta,lik,type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n",
    "     cex.lab=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Properties of Maximum Likelihood Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under certain conditions on the model, the MLE $\\hat{\\theta}_n$ possesses many properties that make it an appealing choice of estimator.\n",
    "\n",
    "The main properties of the MLE are:\n",
    "\n",
    "- It is **consistent**: $\\hat{\\theta}_n \\xrightarrow{\\text{P}} \\theta_*$, where $\\theta_*$ denotes the true value of parameter $\\theta$.\n",
    "- It is **equivariant**: if $\\hat{\\theta}_n$ is the MLE of $\\theta$ then $g(\\hat{\\theta}_n)$ is the MLE of $g(\\theta)$.\n",
    "- If is **asymptotically Normal**: $\\sqrt{n}(\\hat{\\theta} - \\theta_*) / \\hat{\\text{se}} \\leadsto N(0, 1)$ where $\\hat{\\text{se}}$ can be computed analytically.\n",
    "- It is **asymptotically optimal** or **efficient**: roughly, this means that among all well behaved estimators, the MLE has the smallest variance, at least for large samples.\n",
    "- The MLE is approximately the Bayes estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Consistency of Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f$ and $g$ are PDFs, define the **Kullback-Leibler distance** between $f$ and $g$ to be:\n",
    "\n",
    "$$ D(f, g) = \\int f(x) \\log \\left( \\frac{f(x)}{g(x)} \\right) dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that $D(f, g) \\geq 0$ and $D(f, f) = 0$.  For any $\\theta, \\psi \\in \\Theta$ write $D(\\theta, \\psi)$ to mean $D(f(x; \\theta), f(x; \\psi))$.  We will assume that $\\theta \\neq \\psi$ implies $D(\\theta, \\psi) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\theta_*$ denote the true value of $\\theta$.  Maximizing $\\ell_n(\\theta)$ is equivalent to maximizing\n",
    "\n",
    "$$M_n(\\theta) = \\frac{1}{n} \\sum_i \\log \\frac{f(X_i; \\theta)}{f(X_i; \\theta_*)}$$\n",
    "\n",
    "By the law of large numbers, $M_n(\\theta)$ converges to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{\\theta_*} \\left( \\log \\frac{f(X_i; \\theta)}{f(X_i; \\theta_*)} \\right)\n",
    "& = \\int \\log \\left( \\frac{f(x; \\theta)}{f(x; \\theta_*)} \\right) f(x; \\theta_*) dx \\\\\n",
    "& = - \\int \\log \\left( \\frac{f(x; \\theta_*)}{f(x; \\theta)} \\right) f(x; \\theta_*) dx \\\\\n",
    "&= -D(\\theta_*, \\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence $M_n(\\theta) \\approx -D(\\theta_*, \\theta)$ which is maximized at $\\theta_*$, since the KL distance is 0 when $\\theta_* = \\theta$ and positive otherwise.  Hence, we expect that the maximizer will tend to $\\theta_*$.\n",
    "\n",
    "To prove this formally, we need more than $M_n(\\theta) \\xrightarrow{\\text{P}} -D(\\theta_*, \\theta)$.  We need this convergence to be uniform over $\\theta$.  We also have to make sure that the KL distance is well-behaved.  Here are the formal details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.13**.  Let $\\theta_*$ denote the true value of $\\theta$.  Define\n",
    "\n",
    "$$M_n(\\theta) = \\frac{1}{n} \\sum_i \\log \\frac{f(X_i; \\theta)}{f(X_i; \\theta_*)}$$\n",
    "\n",
    "and $M(\\theta) = -D(\\theta_*, \\theta)$.  Suppose that\n",
    "\n",
    "$$ \\sup _{\\theta \\in \\Theta} |M_n(\\theta) - M(\\theta)| \\xrightarrow{\\text{P}} 0 $$\n",
    "\n",
    "and that, for every $\\epsilon > 0$,\n",
    "\n",
    "$$ \\sup _{\\theta : |\\theta - \\theta_*| \\geq \\epsilon} M(\\theta) < M(\\theta_*)$$\n",
    "\n",
    "Let $\\hat{\\theta}_n$ denote the MLE.  Then $\\hat{\\theta}_n \\xrightarrow{\\text{P}} \\theta_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 Equivalence of the MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.14**.  Let $\\tau = g(\\theta)$ be a one-to-one function of $\\theta$.  Let $\\hat{\\theta}_n$ be the MLE of $\\theta$.  Then $\\hat{\\tau}_n = g(\\hat{\\theta}_n)$ is the MLE of $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  Let $h = g^{-1}$ denote the inverse of $g$.  Then $\\hat{\\theta}_n = h(\\hat{\\tau}_n)$.  For any $\\tau$, $L(\\tau) = \\prod_i f(x_i; h(\\tau)) = \\prod_i f(x_i; \\theta) = \\mathcal{L}(\\theta)$ where $\\theta = h(\\tau)$.  Hence, for any $\\tau$, $\\mathcal{L}_n(\\tau) = \\mathcal{L}(\\theta) \\leq \\mathcal{L}(\\hat{\\theta}) = \\mathcal{L}_n(\\hat{\\tau})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 Asymptotic Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **score function** is defined to be\n",
    "\n",
    "$$ s(X; \\theta) = \\frac{\\partial \\log f(X; \\theta)}{\\partial \\theta} $$\n",
    "\n",
    "The **Fisher information** is defined to be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I_n(\\theta) &= \\mathbb{V}_\\theta \\left( \\sum_{i=1}^n s(X_i; \\theta) \\right) \\\\\n",
    "&= \\sum_{i=1}^n \\mathbb{V}_\\theta(s(X_i; \\theta))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $n = 1$ we sometimes write $I(\\theta)$ instead of $I_1(\\theta)$.\n",
    "\n",
    "It can be shown that $\\mathbb{E}_\\theta(s(X; \\theta)) = 0$.  It then follows that $\\mathbb{V}_\\theta(s(X; \\theta)) = \\mathbb{E}_\\theta((s(X; \\theta))^2)$.  A further simplification of $I_n(\\theta)$ is given in the next result.  \n",
    "Note that $\\displaystyle 0\\leq \\mathcal{I}(\\theta)$. A random variable carrying high Fisher information implies that the absolute value of the score is often high. The Fisher information is not a function of a particular observation, as the random variable X has been averaged out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.17**.\n",
    "\n",
    "$$ I_n(\\theta) = n I(\\theta)$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\theta) & = \\mathbb{E}_\\theta \\left( \\frac{\\partial \\log f(X; \\theta)}{\\partial \\theta} \\right)^2 \\\\\n",
    "&= \\int \\left( \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} \\right)^2 f(x; \\theta) dx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $\\logâ€‰f(x; \\theta)$ is twice differentiable with respect to $\\theta$, and under certain regularity conditions, then the Fisher information may also be written as $$\n",
    "\\begin{align}\n",
    "I(\\theta) & = -\\mathbb{E}_\\theta \\left( \\frac{\\partial^2 \\log f(X; \\theta)}{\\partial \\theta^2} \\right) \\\\\n",
    "&= -\\int \\left( \\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2} \\right) f(x; \\theta) dx\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.18 (Asymptotic Normality of the MLE)**.  Under appropriate regularity conditions, the following hold:\n",
    "\n",
    "(1)  Let $\\text{se} = \\sqrt{1 / I_n(\\theta)}$.  Then,\n",
    "\n",
    "$$ \\frac{\\hat{\\theta}_n - \\theta}{\\text{se}} \\leadsto N(0, 1) $$\n",
    "\n",
    "(2) Let $\\hat{\\text{se}} = \\sqrt{1 / I_n(\\hat{\\theta}_n)}$.  Then,\n",
    "\n",
    "$$ \\frac{\\hat{\\theta}_n - \\theta}{\\hat{\\text{se}}} \\leadsto N(0, 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first statement says that $\\hat{\\theta}_n \\approx N(\\theta, \\text{se})$.  The second statement says that this is still true if we replace the standard error $\\text{se}$ by its estimated standard error $\\hat{\\text{se}}$.\n",
    "\n",
    "Informally this says that the distribution of the MLE can be approximated with $N(\\theta, \\hat{\\text{se}})$.  From this fact we can construct an asymptotic confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.19**.  Let\n",
    "\n",
    "$$ C_n = \\left( \\hat{\\theta_n} - z_{\\alpha/2} \\hat{\\text{se}}, \\; \\hat{\\theta_n} + z_{\\alpha/2} \\hat{\\text{se}} \\right) $$\n",
    "\n",
    "Then, $\\mathbb{P}_\\theta(\\theta \\in C_n) \\rightarrow 1 - \\alpha$ as $n \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**  Let $Z$ denote a standard random variable.  Then,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}_\\theta(\\theta \\in C_n) \n",
    "&= \\mathbb{P}_\\theta(\\hat{\\theta}_n - z_{\\alpha/2} \\hat{\\text{se}} \\leq \\theta \\leq \\hat{\\theta}_n + z_{\\alpha/2} \\hat{\\text{se}}) \\\\\n",
    "&= \\mathbb{P}_\\theta(-z_{\\alpha/2} \\leq \\frac{\\hat{\\theta}_n - \\theta}{\\hat{\\text{se}}} \\leq z_{\\alpha/2}) \\\\\n",
    "&\\rightarrow \\mathbb{P}(-z_{\\alpha/2} \\leq Z \\leq z_{\\alpha/2}) = 1 - \\alpha\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be a Bernoulli trial. The Fisher information contained in $X$ may be calculated to be\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\theta) & = -\\mathbb{E} \\left( \\frac{\\partial^2 \\log \\left(\\theta^X(1-\\theta)^{(1-X)}\\right)}{\\partial \\theta^2} \\right) \\\\\n",
    "&= -\\mathbb{E} \\left( \\frac{\\partial^2}{\\partial \\theta^2} \\left[X\\log \\theta+(1-X)\\log(1-\\theta)\\right] \\right) \\\\\n",
    "&=\\mathbb{E}\\left[\\frac{X}{\\theta^2}+\\frac{1-X}{(1-\\theta)^2}\\right] \\\\\n",
    "&=\\frac{\\theta}{\\theta^2}+\\frac{1-\\theta}{(1-\\theta)^2}\\\\\n",
    "&=\\frac{1}{\\theta(1-\\theta)}\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Fisher information is additive, the Fisher information contained in $n$ independent Bernoulli trials is therefore\n",
    "$$I(\\theta)=\\frac{n}{\\theta(1-\\theta)}$$\n",
    "Hence $$\\hat{\\text{se}}=\\frac{1}{\\sqrt{I_n(\\hat{\\theta})}}=\\sqrt{\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}}$$\n",
    "An approximate 95% confidence inteval is $$\\hat{\\theta}_n\\pm2\\sqrt{\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be a Normal trial. \n",
    "The log-likelihood is $$\\ell(\\sigma)=-n\\log\\sigma-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i-\\mu)^2$$\n",
    "Let $$\\ell'(\\sigma)=-n\\frac{1}{\\sigma}+\\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(x_i-\\mu)^2=0$$ get $$\\hat{\\sigma}=\\sqrt{\\frac{\\sum_{i=1}^{n}(x_i-\\mu)^2}{n}}$$\n",
    "To get the Fisher information ,\n",
    "$$\\log f(X;\\sigma)=-\\log\\sigma-\\frac{(X-\\mu)^2}{2\\sigma^2}$$\n",
    "with $$\\frac{\\partial^2\\log f(X;\\sigma)}{\\partial^2\\sigma}=\\frac{1}{\\sigma^2}-\\frac{3(X-\\mu)^2}{\\sigma^4}$$\n",
    "and hence \n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\theta) & = -\\mathbb{E} \\left( \\frac{1}{\\sigma^2}-\\frac{3(X-\\mu)^2}{\\sigma^4} \\right) \\\\\n",
    "&=-\\frac{1}{\\sigma^2}+\\frac{3\\sigma^2}{\\sigma^4}\\\\\n",
    "&=\\frac{2}{\\sigma^2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence $$\\hat{\\text{se}}=\\frac{1}{\\sqrt{I_n(\\hat{\\theta})}}=\\frac{\\hat{\\sigma}_n}{\\sqrt{2n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.8 Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $X_1, \\dots, X_n \\sim N(0, \\sigma^2)$.  The MLE is $\\hat{\\theta}_n = \\overline{X}_n$.  Another reasonable estimator is the sample median $\\overline{\\theta}_n$.  The MLE satisfies\n",
    "\n",
    "$$ \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\leadsto N(0, \\sigma^2) $$\n",
    "\n",
    "It can be proved that the median satisfies\n",
    "\n",
    "$$ \\sqrt{n}(\\overline{\\theta}_n - \\theta) \\leadsto N\\left(0, \\sigma^2 \\frac{\\pi}{2} \\right) $$\n",
    "\n",
    "This means that the median converges to the right value but has a larger variance than the MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, consider two estimators $T_n$ and $U_n$ and suppose that\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(T_n - \\theta) \\leadsto N(0, t^2) \n",
    "\\quad \\text{and} \\quad \n",
    "\\sqrt{n}(U_n - \\theta) \\leadsto N(0, u^2)\n",
    "$$\n",
    "\n",
    "We define the **asymptotic relative efficiency** of U to T by $ARE(U, T) = t^2/u^2$.  In the Normal example, $ARE(\\overline{\\theta}_n, \\hat{\\theta}_n) = 2 / \\pi = 0.63$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.23**.  If $\\hat{\\theta}_n$ is the MLE and $\\widetilde{\\theta}_n$ is any other estimator then\n",
    "\n",
    "$$ ARE(\\widetilde{\\theta}_n, \\hat{\\theta}_n) \\leq 1 $$\n",
    "\n",
    "Thus, MLE has the smallest (asymptotic) variance and we say that MLE is **efficient** or **asymptotically optimal**.\n",
    "\n",
    "The result is predicated over the model being correct -- otherwise the MLE may no longer be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.9 The Delta Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\tau = g(\\theta)$ where $g$ is a smooth function.  The maximum likelihood estimator of $\\tau$ is $\\hat{\\tau} = g(\\hat{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.24 (The Delta Method)**.  If $\\tau = g(\\theta)$ where $g$ is differentiable and $g'(\\theta) \\neq 0$ then\n",
    "\n",
    "$$ \\frac{\\sqrt{n}(\\hat{\\tau}_n - \\tau)}{\\hat{\\text{se}}(\\hat{\\tau})} \\leadsto N(0, 1) $$\n",
    "\n",
    "where $\\hat{\\tau}_n = g(\\hat{\\theta})$ and\n",
    "\n",
    "$$ \\hat{\\text{se}}(\\hat{\\tau}_n) = |g'(\\hat{\\theta})| \\hat{\\text{se}} (\\hat{\\theta}_n) $$\n",
    "\n",
    "Hence, if\n",
    "\n",
    "$$ C_n = \\left( \\hat{\\tau}_n - z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\tau}_n), \\; \\hat{\\tau}_n + z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\tau}_n) \\right) $$\n",
    "\n",
    "then $\\mathbb{P}_\\theta(\\tau \\in C_n) \\rightarrow 1 - \\alpha$ as $n \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.10 Multiparameter Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend these ideas to models with several parameters.\n",
    "\n",
    "Let $\\theta = (\\theta_1, \\dots, \\theta_n)$ and let $\\hat{\\theta} = (\\hat{\\theta}_1, \\dots, \\hat{\\theta}_n)$ be the MLE.  Let $\\ell_n = \\sum_{i=1}^n \\log f(X_i; \\theta)$,\n",
    "\n",
    "$$\n",
    "H_{jj} = \\frac{\\partial^2 \\ell_n}{\\partial \\theta_j^2}\n",
    "\\quad \\text{and} \\quad\n",
    "H_{jk} = \\frac{\\partial^2 \\ell_n}{\\partial \\theta_j \\partial \\theta_k}\n",
    "$$\n",
    "\n",
    "Define the **Fisher Information Matrix** by\n",
    "\n",
    "$$\n",
    "I_n(\\theta) = -\n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}_\\theta(H_{11}) & \\mathbb{E}_\\theta(H_{12}) & \\cdots & \\mathbb{E}_\\theta(H_{1k}) \\\\\n",
    "\\mathbb{E}_\\theta(H_{21}) & \\mathbb{E}_\\theta(H_{22}) & \\cdots & \\mathbb{E}_\\theta(H_{2k}) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbb{E}_\\theta(H_{k1}) & \\mathbb{E}_\\theta(H_{k2}) & \\cdots & \\mathbb{E}_\\theta(H_{kk})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let $J_n(\\theta) = I_n^{-1}(\\theta)$ be the inverse of $I_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.27**. Under appropriate regularity conditions,\n",
    "\n",
    "$$ \\sqrt{n}(\\hat{\\theta} - \\theta) \\approx N(0, J_n(\\theta))$$\n",
    "\n",
    "Also, if $\\hat{\\theta}_j$ is the $j$-th component of $\\hat{\\theta}$, then\n",
    "\n",
    "$$ \\frac{\\sqrt{n}(\\hat{\\theta_j} - \\theta_j)}{\\hat{\\text{se}}_j} \\approx N(0, 1) $$\n",
    "\n",
    "where $\\hat{\\text{se}}_j^2$ is the $j$-th diagonal element of $J_n$.  The approximate covariance of $\\hat{\\theta}_j$ and $\\hat{\\theta}_k$ is $\\text{Cov}(\\hat{\\theta}_j, \\hat{\\theta}_k) \\approx J_n(j, k)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a multiparameter delta method.  Let $\\tau = g(\\theta_1, \\dots, \\theta_k)$ be a function and let\n",
    "\n",
    "$$ \\nabla g = \\begin{pmatrix}\n",
    "\\frac{\\partial g}{\\partial \\theta_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial g}{\\partial \\theta_k}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "be the gradient of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.28 (Multiparameter delta method)**.  Suppose that $\\nabla g$ evaluated at $\\hat{\\theta}$ is not 0. Let $\\hat{\\tau} = g(\\hat{\\theta})$.  Then\n",
    "\n",
    "$$ \\frac{\\sqrt{n}(\\hat{\\tau} - \\tau)}{\\hat{\\text{se}}(\\hat{\\tau})} \\leadsto N(0, 1) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\hat{\\text{se}}(\\hat{\\tau}) = \\sqrt{\\left(\\hat{\\nabla} g \\right)^T \\hat{J}_n \\left(\\hat{\\nabla} g \\right)} ,$$\n",
    "\n",
    "$\\hat{J}_n = J_n(\\hat{\\theta}_n)$ and $\\hat{\\nabla}g$ is $\\nabla g$ evaluated at $\\theta = \\hat{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.11 The Parametric Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parametric models, standard errors and confidence intervals may also be estimated using the bootstrap.  There is only one change.  In nonparametric bootstrap, we sampled $X_1^*, \\dots, X_n*$ from the empirical distribution $\\hat{F}_n$. In the parametric bootstrap we sample instead from $f(x; \\hat{\\theta}_n)$.  Here, $\\hat{\\theta}_n$ could be the MLE or the method of moments estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.12 Technical Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.12.1 Proofs\n",
    "**Theorem 10.13**.  Let $\\theta_*$ denote the true value of $\\theta$.  Define\n",
    "\n",
    "$$M_n(\\theta) = \\frac{1}{n} \\sum_i \\log \\frac{f(X_i; \\theta)}{f(X_i; \\theta_*)}$$\n",
    "\n",
    "and $M(\\theta) = -D(\\theta_*, \\theta)$.  Suppose that\n",
    "\n",
    "$$ \\sup _{\\theta \\in \\Theta} |M_n(\\theta) - M(\\theta)| \\xrightarrow{\\text{P}} 0 $$\n",
    "\n",
    "and that, for every $\\epsilon > 0$,\n",
    "\n",
    "$$ \\sup _{\\theta : |\\theta - \\theta_*| \\geq \\epsilon} M(\\theta) < M(\\theta_*)$$\n",
    "\n",
    "Let $\\hat{\\theta}_n$ denote the MLE.  Then $\\hat{\\theta}_n \\xrightarrow{\\text{P}} \\theta_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof of Theorem 10.13**.  Since $\\hat{\\theta}_n$ maximizes $M_n(\\theta)$, we have $M_n(\\hat{\\theta}) \\geq M_n(\\theta_*)$.  Hence,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "M(\\theta_*) - M(\\hat{\\theta}_n) \n",
    "&= M_n(\\theta_*) - M(\\hat{\\theta}_n) + M(\\hat{\\theta}_*) - M_n(\\theta_*) \\\\\n",
    "&\\leq M_n(\\hat{\\theta}) - M(\\hat{\\theta}_n) + M(\\theta_*) - M_n(\\theta_*) \\\\\n",
    "&\\leq \\sup_\\theta | M_n(\\theta) - M(\\theta) |  + M(\\theta_*)  - M_n(\\theta_*) \\\\\n",
    "&\\xrightarrow{\\text{P}} 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It follows that, for any $\\delta > 0$, \n",
    "\n",
    "$$\\mathbb{P}(M(\\hat{\\theta}_n) < M(\\theta_*) - \\delta) \\rightarrow 0$$\n",
    "\n",
    "Pick any $\\epsilon > 0$.  There exists $\\delta > 0$ such that $|\\theta - \\theta_*| \\geq \\epsilon$ implies that $M(\\theta) < M(\\theta_*) - \\delta$.  Hence,\n",
    "\n",
    "$$\\mathbb{P}(|\\hat{\\theta}_n - \\theta_*| > \\epsilon) \\leq \n",
    "\\mathbb{P}\\left( M(\\hat{\\theta}_n) < M(\\theta_*) - \\delta \\right) \\rightarrow 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 10.31**.  The score function satisfies\n",
    "\n",
    "$$\\mathbb{E}[s(X; \\theta)] = 0$$\n",
    "\n",
    "**Proof**.  Note that $1 = \\int f(x; \\theta) dx$.  Differentiate both sides of this equation to get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "0 &= \\frac{\\partial}{\\partial \\theta} \\int f(x; \\theta)dx = \\int \\frac{\\partial}{\\partial \\theta} f(x; \\theta) dx \\\\\n",
    "&= \\int \\frac{\\frac{\\partial f(x; \\theta)}{\\partial \\theta}}{f(x; \\theta)} f(x; \\theta) dx\n",
    "= \\int \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} f(x; \\theta) dx \\\\\n",
    "&= \\int s(x; \\theta) f(x; \\theta) dx = \\mathbb{E}[s(X; \\theta)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.18 (Asymptotic Normality of the MLE)**.  Under appropriate regularity conditions, the following hold:\n",
    "\n",
    "(1)  Let $\\text{se} = \\sqrt{1 / I_n(\\theta)}$.  Then,\n",
    "\n",
    "$$ \\frac{\\hat{\\theta}_n - \\theta}{\\text{se}} \\leadsto N(0, 1) $$\n",
    "\n",
    "(2) Let $\\hat{\\text{se}} = \\sqrt{1 / I_n(\\hat{\\theta}_n)}$.  Then,\n",
    "\n",
    "$$ \\frac{\\hat{\\theta}_n - \\theta}{\\hat{\\text{se}}} \\leadsto N(0, 1) $$\n",
    "\n",
    "\n",
    "**Proof of Theorem 10.18**.  Let $\\ell(\\theta) = \\log \\mathcal{L}(\\theta)$.  Then\n",
    "\n",
    "$$0 = \\ell'(\\hat{\\theta}) \\approx \\ell'(\\theta) + (\\hat{\\theta} - \\theta) \\ell''(\\theta)$$\n",
    "\n",
    "Rearrange the above equation to get $\\hat{\\theta} - \\theta = -\\ell'(\\theta) / \\ell''(\\theta)$, or\n",
    "\n",
    "$$ \\sqrt{n}(\\hat{\\theta} - \\theta) = \\frac{\\frac{1}{\\sqrt{n}}\\ell'(\\theta)}{-\\frac{1}{n}\\ell''(\\theta)} = \\frac{\\text{TOP}}{\\text{BOTTOM}}$$\n",
    "\n",
    "Let $Y_i = \\partial \\log f(X_i, \\theta) / \\partial \\theta$.  From the previous lemma $\\mathbb{E}(Y_i) = 0$ and also $\\mathbb{V}(Y_i) = I(\\theta)$.  Hence,\n",
    "\n",
    "$$\\text{TOP} = n^{-1/2} \\sum_i Y_i = \\sqrt{n} \\overline{Y} = \\sqrt{n} (\\overline{Y} - 0) \\leadsto W \\sim N(0, I)$$\n",
    "\n",
    "by the central limit theorem.  Let $A_i = -\\partial^2 \\log f(X_i; \\theta) / \\partial theta^2$.  Then $\\mathbb{E}(A_i) = I(\\theta)$ and\n",
    "\n",
    "$$\\text{BOTTOM} = \\overline{A} \\xrightarrow{\\text{P}} I(\\theta)$$\n",
    "\n",
    "by the law of large numbers.  Apply Theorem 6.5 part (e) to conclude that\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\theta} - \\theta) \\leadsto \\frac{W}{I(\\theta)} \\sim N \\left(0, \\frac{1}{I(\\theta)} \\right)$$\n",
    "\n",
    "Assuming that $I(\\theta)$ is a continuous function of $\\theta$, it follows that $I(\\hat{\\theta}_n) \\xrightarrow{\\text{P}} I(\\theta)$.  Now\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\hat{\\theta}_n - \\theta}{\\hat{\\text{se}}}&= \\sqrt{n} I^{1/2}(\\hat{\\theta_n})(\\hat{\\theta_n} - \\theta) \\\\\n",
    "&= \\left\\{ \\sqrt{n} I^{1/2}(\\theta)(\\hat{\\theta}_n - \\theta)\\right\\} \\left\\{ \\frac{I(\\hat{\\theta}_n)}{I(\\theta)} \\right\\}^{1/2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The first term tends in distribution to $N(0, 1)$.  The second term tends in probability to 1.  The result follows from Theorem 6.5 part (e)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem 10.24 (The Delta Method)**.  If $\\tau = g(\\theta)$ where $g$ is differentiable and $g'(\\theta) \\neq 0$ then\n",
    "\n",
    "$$ \\frac{\\sqrt{n}(\\hat{\\tau}_n - \\tau)}{\\hat{\\text{se}}(\\hat{\\tau})} \\leadsto N(0, 1) $$\n",
    "\n",
    "where $\\hat{\\tau}_n = g(\\hat{\\theta})$ and\n",
    "\n",
    "$$ \\hat{\\text{se}}(\\hat{\\tau}_n) = |g'(\\hat{\\theta})| \\hat{\\text{se}} (\\hat{\\theta}_n) $$\n",
    "\n",
    "Hence, if\n",
    "\n",
    "$$ C_n = \\left( \\hat{\\tau}_n - z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\tau}_n), \\; \\hat{\\tau}_n + z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\tau}_n) \\right) $$\n",
    "\n",
    "then $\\mathbb{P}_\\theta(\\tau \\in C_n) \\rightarrow 1 - \\alpha$ as $n \\rightarrow \\infty$.\n",
    "\n",
    "**Outline of proof of Theorem 10.24**.  Write,\n",
    "\n",
    "$$\\hat{\\tau} = g(\\hat{\\theta}) \\approx g(\\theta) + (\\hat{\\theta} - \\theta)g'(\\theta) = \\tau + (\\hat{\\theta} - \\theta)g'(\\theta)$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\tau} - \\tau) \\approx \\sqrt{n}(\\hat{\\theta} - \\theta)g'(\\theta)$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\\frac{\\sqrt{n}I^{1/2}(\\theta)(\\hat{\\theta} - \\theta)}{g'(\\theta)} \\approx \\sqrt{n}I^{1/2}(\\theta)(\\hat{\\theta} - \\theta)$$\n",
    "\n",
    "Theorem 10.18 tells us that the right hand side tends in distribution to $N(0, 1)$, hence\n",
    "\n",
    "$$\\frac{\\sqrt{n}I^{1/2}(\\theta)(\\hat{\\theta} - \\theta)}{g'(\\theta)} \\leadsto N(0, 1)$$\n",
    "\n",
    "or, in other words,\n",
    "\n",
    "$$\\hat{\\tau} \\approx N(\\tau, \\text{se}^2(\\hat{\\tau}_n))$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\text{se}^2(\\hat{\\tau}_n) = \\frac{(g'(\\theta))^2}{nI(\\theta)}$$\n",
    "\n",
    "The result remains true if we substitute $\\hat{\\theta}$ for $\\theta$ by Theorem 6.5 part (e)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.12.2 Sufficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **statistic** is a function $T(X^n)$ of the data.  A sufficient statistic is a statistic that contains all of the information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write $x^n \\leftrightarrow y^n$ if $f(x^n; \\theta) = c f(y^n; \\theta)$ for some constant $c$ that might depend on $x^n$ and $y^n$ but not $\\theta$.  A statistic is **sufficient** if $T(x^n) \\leftrightarrow T(y^n)$ implies that $x^n \\leftrightarrow y^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if $x^n \\leftrightarrow y^n$ then the likelihood functions based on $x^n$ and $y^n$ have the same shape.  Roughly speaking, a statistic is sufficient if we can calculate the likelihood function knowing only $T(X^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistic $T$ is **minimally sufficient** if it is sufficient and it is a function of every other sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.36**. $T$ is minimally sufficient if $T(x^n) = T(y^n)$ if and only if $x^n \\leftrightarrow y^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual definition of sufficiency is this: $T$ is sufficient if the distribution of $X^n$ given $T(X^n) = t$ does not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.40 (Factorization Theorem)**.  $T$ is sufficient if and only if there are functions $g(t, \\theta)$ and $h(x)$ such that $f(x^n; \\theta) = g(t(x^n); \\theta)h(x^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.42 (Rao-Blackwell)**.  Let $\\hat{\\theta}$ be an estimator and let $T$ be a sufficient statistic.  Define a new estimator by\n",
    "\n",
    "$$\\overline{\\theta} = \\mathbb{E}(\\hat{\\theta} | T)$$\n",
    "\n",
    "Then, for every $\\theta$, \n",
    "\n",
    "$$R(\\theta, \\overline{\\theta}) \\leq R(\\theta, \\hat{\\theta})$$\n",
    "\n",
    "where $R(\\theta, \\hat{\\theta}) = \\mathbb{E}_\\theta[(\\theta - \\hat{\\theta})^2]$ denote the MSE of an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.12.3 Exponential Families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that $\\{f(x; \\theta) : \\theta \\in \\Theta\\}$ is a **one-parameter exponential family** if there are functions $\\eta(\\theta)$, $B(\\theta)$, $T(x)$ and $h(x)$ such that\n",
    "\n",
    "$$f(x; \\theta) = h(x) e^{\\eta(\\theta)T(x) - B(\\theta)}$$\n",
    "\n",
    "It is easy to see that $T(X)$ is sufficient.  We call $T$ the **natural sufficient statistic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite an exponential family as\n",
    "\n",
    "$$f(x; \\eta) = h(x) e^{\\eta T(x) - A(\\eta)}$$\n",
    "\n",
    "where $\\eta = \\eta(\\theta)$ is called the **natural parameter** and\n",
    "\n",
    "$$A(\\eta) = \\log \\int h(x) e^{\\eta T(x)} dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_1, \\dots, X_n$ be iid from an exponential family. Then $f(x^n; \\theta)$ is an exponential family:\n",
    "\n",
    "$$f(x^n; \\theta) = h_n(x^n) e^{\\eta(\\theta) T_n(x^n) - B_n(\\theta)}$$\n",
    "\n",
    "where $h_n(x^n) = \\prod_i h(x_i)$, $T_n(x^n) = \\sum_i T(x_i)$ and $B_n(\\theta) = nB(\\theta)$.  This implies that $\\sum_i T(X_i)$ is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 10.47**. Let $X$ have an exponential family.  Then,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(T(X)) = A'(\\eta),\n",
    "\\quad\n",
    "\\mathbb{V}(T(X)) = A''(\\eta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\theta = (\\theta_1, \\dots, \\theta_n)$ is a vector, then we say that $f(x; \\theta)$ has exponential family form if\n",
    "\n",
    "$$ f(x; \\theta) = h(x) \\exp \\left\\{ \\sum_{j=1}^k \\eta_j(\\theta) T_j(x) - B(\\theta) \\right\\}$$\n",
    "\n",
    "Again, $T = (T_1, \\dots, T_k)$ is sufficient and $n$ iid samples also has exponential form with sufficient statistic $\\left(\\sum_i T_1(X_i), \\dots, \\sum_i T_k(X_i)\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.13 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.1**.  Let $X_1, \\dots, X_n \\sim \\text{Gamma}(\\alpha, \\beta)$.  Find the method of moments estimator for $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**. \n",
    "\n",
    "Let $X \\sim \\text{Gamma}(\\alpha, \\beta)$.  The PDF is \n",
    "\n",
    "$$ f_X(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\beta x} \\quad \\text{for } x > 0 $$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathbb{E}(X) \n",
    "&= \\int x f_X(x) dx \\\\\n",
    "&= \\int_0^\\infty x \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\beta x} dx \\\\\n",
    "&= \\frac{\\alpha}{\\beta} \\int_0^\\infty\\frac{\\beta^{\\alpha + 1}}{\\Gamma(\\alpha + 1)} x^\\alpha e^{-\\beta x} dx \\\\\n",
    "&= \\frac{\\alpha}{\\beta}\n",
    "\\end{align}\n",
    "$$\n",
    "We also have:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathbb{E}(X^2) \n",
    "&= \\int x^2 f_X(x) dx \\\\\n",
    "&= \\int_0^\\infty x^2 \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\beta x} dx \\\\\n",
    "&= \\frac{\\alpha (\\alpha + 1)}{\\beta^2} \\int_0^\\infty\\frac{\\beta^{\\alpha + 2}}{\\Gamma(\\alpha + 2)} x^{\\alpha + 1} e^{-\\beta x} dx \\\\\n",
    "&= \\frac{\\alpha (\\alpha + 1)}{\\beta^2}\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore,\n",
    "\n",
    "$$ \\mathbb{V}(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 = \\frac{\\alpha (\\alpha + 1)}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} = \\frac{\\alpha}{\\beta^2} $$\n",
    "\n",
    "The first two moments are:\n",
    "\n",
    "$$\n",
    "\\alpha_1 = \\mathbb{E}(X) = \\frac{\\alpha}{\\beta}\\\\\n",
    "\\alpha_2 = \\mathbb{E}(X^2) = \\mathbb{V}(X) + \\mathbb{E}(X)^2 = \\frac{\\alpha}{\\beta^2} + \\frac{\\alpha^2}{\\beta^2} = \\frac{\\alpha(\\alpha + 1)}{\\beta^2} \n",
    "$$\n",
    "\n",
    "We have the sample moments:\n",
    "\n",
    "$$\\hat{\\alpha}_1 = \\frac{1}{n}\\sum_{i=1}^n X_i\n",
    "\\quad \\quad\n",
    "\\hat{\\alpha}_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2$$\n",
    "\n",
    "Equating these we get:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_1 = \\frac{\\hat{\\alpha}_n}{\\hat{\\beta}_n}\n",
    "\\quad \\quad\n",
    "\\hat{\\alpha}_2 = \\frac{\\hat{\\alpha}_n(\\hat{\\alpha}_n + 1)}{\\hat{\\beta}_n^2}\n",
    "$$\n",
    "\n",
    "Solving these we get the method of moments estimators for $\\alpha$ and $\\beta$:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_n = \\frac{\\hat{\\alpha}_1^2}{\\hat{\\alpha}_2 - \\hat{\\alpha}_1^2}\n",
    "\\quad \\quad\n",
    "\\hat{\\beta}_n = \\frac{\\hat{\\alpha}_1}{\\hat{\\alpha}_2 - \\hat{\\alpha}_1^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.2**. Let $X_1, \\dots, X_n \\sim \\text{Uniform}(a, b)$ where $a, b$ are unknown parameters and $a < b$.\n",
    "\n",
    "**(a)** Find the method of moments estimators for $a$ and $b$.\n",
    "\n",
    "**(b)** Find the MLE $\\hat{a}$ and $\\hat{b}$.\n",
    "\n",
    "**(c)** Let $\\tau = \\int x dF(x)$.  Find the MLE of $\\tau$.\n",
    "\n",
    "**(d)** Let $\\hat{\\tau}$ be the MLE from the previous item.  Let $\\tilde{\\tau}$ be the nonparametric plug-in estimator of $\\tau = \\int x dF(x)$.  Suppose that $a = 1$, $b = 3$ and $n = 10$.  Find the MSE of $\\hat{\\tau}$ by simulation.  Find the MSE of $\\tilde{\\tau}$ analytically.  Compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "**(a)**\n",
    "\n",
    "\n",
    "Let $X \\sim \\text{Uniform}(a, b)$.  Then:\n",
    "\n",
    "- $$\\mathbb{E}(X) = \\int_a^b x \\frac{1}{b - a} dx = \\frac{a + b}{2}$$\n",
    "- $$\\mathbb{E}(X^2) = \\int_a^b x^2 \\frac{1}{b - a} dx = \\frac{a^2 + ab + b^2}{3}$$\n",
    "- $$\\mathbb{V}(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 = \\frac{a^2 + ab + b^2}{3} - \\frac{a^2 + 2ab + b^2}{4} = \\frac{(b - a)^2}{12}$$\n",
    "The first two moments are:\n",
    "\n",
    "$$ \n",
    "\\alpha_1 = \\mathbb{E}(X) = \\frac{a + b}{2} \\\\\n",
    "\\alpha_2 = \\mathbb{E}(X^2) = \\mathbb{V}(X) + \\mathbb{E}(X)^2 = \\frac{(b - a)^2}{12} + \\frac{(a + b)^2}{4}\n",
    "= \\frac{a^2 + ab + b^2}{3}\n",
    "$$\n",
    "\n",
    "We have the sample moments:\n",
    "\n",
    "$$\\hat{\\alpha}_1 = \\frac{1}{n}\\sum_{i=1}^n X_i\n",
    "\\quad \\quad\n",
    "\\hat{\\alpha}_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2$$\n",
    "\n",
    "Equating these we get:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_1 = \\frac{\\hat{a} + \\hat{b}}{2}\n",
    "\\quad \\quad\n",
    "\\hat{\\alpha}_2 = \\frac{(\\hat{b} - \\hat{a})^2}{12} + \\frac{(\\hat{a} + \\hat{b})^2}{4}\n",
    "$$\n",
    "\n",
    "Solving these we get the method of moment estimators for $a$ and $b$:\n",
    "\n",
    "$$\n",
    "\\hat{a} = \\hat{\\alpha}_1 - \\sqrt{3}(\\hat{\\alpha}_1^2 - \\hat{\\alpha}_2)\n",
    "\\quad \\quad\n",
    "\\hat{b} = \\hat{\\alpha}_1 + \\sqrt{3}(\\hat{\\alpha}_1^2 - \\hat{\\alpha}_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "The probability density function for each $X_i$ is\n",
    "\n",
    "$$f(x; (a, b)) = \\begin{cases}\n",
    "(b - a)^{-1} & \\text{if } a \\leq x \\leq b \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "The likelihood function is\n",
    "\n",
    "$$\\mathcal{L}_n(a, b) = \\prod_{i=1}^n f(X_i; (a, b)) = \\begin{cases}\n",
    "(b-a)^{-n} & \\text{if } a \\leq X_i \\leq b \\text{ for all } X_i\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The parameters that maximize the likelihood function make the $b - a$ as small as possible -- that is, we should pick the maximum $a$ and the minimum $b$ for which the likelihood function is non-zero.  So the MLEs are:\n",
    "\n",
    "$$\\hat{a} = \\min \\{X_1, \\dots, X_n \\}\n",
    "\\quad \\quad\n",
    "\\hat{b} = \\max \\{X_1, \\dots, X_n \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "$\\tau = \\int x dF(x) = \\mathbb{E}(x) = (a + b)/2$, so since the MLE is equivariant, the MLE of $\\tau$ is \n",
    "\n",
    "$$\\hat{\\tau} = \\frac{\\hat{a} + \\hat{b}}{2} = \\frac{\\min \\{X_1, \\dots, X_n\\} + \\max\\{X_1, \\dots, X_n\\}}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = 1\n",
    "b = 3\n",
    "n = 10\n",
    "\n",
    "X = np.random.uniform(low=a, high=b, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for tau_hat: \t 0.091\n"
     ]
    }
   ],
   "source": [
    "tau_hat = (X.min() + X.max()) / 2\n",
    "\n",
    "# Nonparametric bootstrap to find MSE of tau_hat\n",
    "B = 10000\n",
    "t_boot = np.empty(B)\n",
    "for i in range(B):\n",
    "    xx = np.random.choice(X, n, replace=True)\n",
    "    t_boot[i] = (xx.min() + xx.max()) / 2\n",
    "    \n",
    "se = t_boot.std()\n",
    "print(\"MSE for tau_hat: \\t %.3f\" % se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytically, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{V}(\\tilde{\\tau}) \n",
    "&= \\mathbb{E}(\\tilde{\\tau}^2) - (\\mathbb{E}(\\tilde{\\tau}))^2 \\\\\n",
    "&= \\frac{1}{n^2}\\left(\\mathbb{E}\\left[ \\left(\\sum_{i=1}^n X_i\\right)^2\\right] - \\left(\\mathbb{E}\\left[\\sum_{i=1}^n X_i \\right]\\right)^2\\right) \\\\\n",
    "&= \\frac{1}{n^2}\\left( \\mathbb{E}\\left[ \\sum_{i=1}^n X_i^2 + \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n X_i X_j \\right] - \\left(n \\frac{a + b}{2}\\right)^2\\right) \\\\\n",
    "&= \\frac{1}{n^2}\\left( \\sum_{i=1}^n \\mathbb{E}[X_i^2] + \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\mathbb{E}[X_i]\\mathbb{E}[X_j]  - \\left(n \\frac{a + b}{2}\\right)^2\\right) \\\\\n",
    "&= \\frac{1}{n^2}\\left( n \\frac{a^2 + ab + b^2}{3} + n(n-1) \\left(\\frac{a+b}{2}\\right)^2  - n^2\\left(\\frac{a + b}{2}\\right)^2\\right) \\\\\n",
    "&= \\frac{1}{n^2}\\left( n \\frac{a^2 + ab + b^2}{3} - n \\left(\\frac{a+b}{2}\\right)^2 \\right) \\\\\n",
    "&= \\frac{1}{n} \\left( \\frac{a^2 + ab + b^2}{3} - \\frac{a^2 + 2ab + b^2}{4}\\right) \\\\\n",
    "&= \\frac{1}{n} \\frac{(b - a)^2}{12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\text{se}(\\tilde{\\tau}) = \\sqrt{\\frac{1}{n} \\frac{(b - a)^2}{12}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for tau_tilde: \t 0.183\n"
     ]
    }
   ],
   "source": [
    "se_tau_tilde = np.sqrt((1/n) * ((b - a)**2 / 12))\n",
    "\n",
    "print(\"MSE for tau_tilde: \\t %.3f\" % se_tau_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.3**.  Let $X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)$.  Let $\\tau$ be the 0.95 percentile, i.e. $\\mathbb{P}(X < \\tau) = 0.95$.\n",
    "\n",
    "**(a)** Find the MLE of $\\tau$.\n",
    "\n",
    "**(b)** Find an expression for an approximate $1 - \\alpha$ confidence interval for $\\tau$.\n",
    "\n",
    "**(c)** Suppose the data are:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "3.23 & -2.50 &  1.88 & -0.68 &  4.43 & 0.17 \\\\ \n",
    "1.03 & -0.07 & -0.01 &  0.76 &  1.76 & 3.18 \\\\\n",
    "0.33 & -0.31 &  0.30 & -0.61 &  1.52 & 5.43 \\\\\n",
    "1.54 &  2.28 &  0.42 &  2.33 & -1.03 & 4.00 \\\\\n",
    "0.39 \n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Find the MLE $\\hat{\\tau}$.  Find the standard error using the delta method.  Find the standard error using the parametric bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  \n",
    "\n",
    "Let $Z \\sim N(0, 1)$, so $(X - \\mu) / \\sigma \\sim Z$.  We have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(X < \\tau) &= 0.95 \\\\\n",
    "\\mathbb{P}\\left(\\frac{X - \\mu}{\\sigma} < \\frac{\\tau - \\mu}{\\sigma}\\right) &= 0.95 \\\\\n",
    "\\mathbb{P}\\left(Z < \\frac{\\tau - \\mu}{\\sigma}\\right) &= 0.95 \\\\\n",
    "\\frac{\\tau - \\mu}{\\sigma} &= z_{5\\%} \\\\\n",
    "\\tau &= \\mu + z_{5\\%} \\sigma \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the MLE is equivariant, $\\hat{\\tau} = \\hat{\\mu} + z_{5\\%} \\hat{\\sigma}$, where $\\hat{\\mu}, \\hat{\\sigma}$ are the MLEs for the Normal distribution parameters:\n",
    "\n",
    "$$ \\hat{\\mu} = n^{-1} \\sum_{i=1}^n X_i\n",
    "\\quad \\quad\n",
    "\\hat{\\sigma} = \\sqrt{n^{-1} \\sum_{i=1}^n (X_i - \\overline{X})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "Let's use the multiparameter delta method.  \n",
    "\n",
    "We have $\\tau = g(\\mu, \\sigma) = \\mu + z_{5\\%} \\sigma$, so\n",
    "\n",
    "$$ \\nabla g = \\begin{bmatrix}\n",
    "\\partial g / \\partial \\mu \\\\\n",
    "\\partial g / \\partial \\sigma\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 \\\\\n",
    "z_{5\\%}\n",
    "\\end{bmatrix}$$.\n",
    "\n",
    "\n",
    "Let $X$ be a Normal trial. \n",
    "The log-likelihood is $$\\ell(\\sigma)=-n\\log\\sigma-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i-\\mu)^2$$\n",
    "Let $$\\ell'(\\sigma)=-n\\frac{1}{\\sigma}+\\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(x_i-\\mu)^2=0$$ get $$\\hat{\\sigma}=\\sqrt{\\frac{\\sum_{i=1}^{n}(x_i-\\mu)^2}{n}}$$\n",
    "To get the Fisher information ,\n",
    "$$\\log f(X;\\sigma)=-\\log\\sigma-\\frac{(X-\\mu)^2}{2\\sigma^2}$$\n",
    "with $$\\frac{\\partial^2\\log f(X;\\sigma)}{\\partial^2\\sigma}=\\frac{1}{\\sigma^2}-\\frac{3(X-\\mu)^2}{\\sigma^4}$$\n",
    "and hence \n",
    "\n",
    "$$I(\\mu)=\\frac{1}{\\sigma^2}$$\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\sigma) & = -\\mathbb{E} \\left( \\frac{1}{\\sigma^2}-\\frac{3(X-\\mu)^2}{\\sigma^4} \\right) \\\\\n",
    "&=-\\frac{1}{\\sigma^2}+\\frac{3\\sigma^2}{\\sigma^4}\\\\\n",
    "&=\\frac{2}{\\sigma^2}\n",
    "\\end{align}\n",
    "$$\n",
    "The Fisher Information Matrix for the Normal process is\n",
    "\n",
    "$$ I_n(\\mu, \\sigma) = \\begin{bmatrix}\n",
    "n / \\sigma^2 & 0 \\\\\n",
    "0 & 2n / \\sigma^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then its inverse is\n",
    "\n",
    "$$ J_n = I_n^{-1}(\\mu, \\sigma) = \\frac{1}{n} \\begin{bmatrix}\n",
    "\\sigma^2 & 0 \\\\\n",
    "0 & \\sigma^2/2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the standard error estimate for our new parameter variable is\n",
    "\n",
    "$$\\hat{\\text{se}}(\\hat{\\tau}) = \\sqrt{(\\hat{\\nabla} g)^T \\hat{J}_n (\\hat{\\nabla} g)} = \\hat{\\sigma} \\sqrt{n^{-1}(1 + z_{5\\%}^2 / 2)}$$\n",
    "\n",
    "A $1 - \\alpha$ confidence interval for $\\hat{\\tau}$, then, is\n",
    "\n",
    "$$ C_n = \\left(\n",
    "\\hat{\\mu} + \\hat{\\sigma}\\left( z_{5\\%} - z_{\\alpha / 2} \\sqrt{n^{-1}(1 + z_{5\\%}^2 / 2)} \\right), \\;\n",
    "\\hat{\\mu} + \\hat{\\sigma}\\left( z_{5\\%} + z_{\\alpha / 2} \\sqrt{n^{-1}(1 + z_{5\\%}^2 / 2)} \\right) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "z_05 = norm.ppf(0.95)\n",
    "z_025 = norm.ppf(0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    3.23, -2.50,  1.88, -0.68,  4.43, 0.17,\n",
    "    1.03, -0.07, -0.01,  0.76,  1.76, 3.18,\n",
    "    0.33, -0.31,  0.30, -0.61,  1.52, 5.43,\n",
    "    1.54,  2.28,  0.42,  2.33, -1.03, 4.00,\n",
    "    0.39   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau: 4.180\n"
     ]
    }
   ],
   "source": [
    "# Estimate the MLE tau_hat\n",
    "\n",
    "n = len(X)\n",
    "mu_hat = X.mean()\n",
    "sigma_hat = X.std()\n",
    "tau_hat = mu_hat + z_05 * sigma_hat\n",
    "\n",
    "print(\"Estimated tau: %.3f\" % tau_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau (delta method, 95% confidence interval): \t (3.088, 5.273)\n"
     ]
    }
   ],
   "source": [
    "# Confidence interval using delta method\n",
    "\n",
    "se_tau_hat = sigma_hat * np.sqrt((1/n) * (1 + z_05 * z_05 / 2))\n",
    "confidence_interval = (tau_hat - z_025 * se_tau_hat, tau_hat + z_025 * se_tau_hat)\n",
    "\n",
    "print(\"Estimated tau (delta method, 95%% confidence interval): \\t (%.3f, %.3f)\" % confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau (parametric bootstrap, 95% confidence interval): \t (2.898, 5.463)\n"
     ]
    }
   ],
   "source": [
    "# Confidence interval using parametric bootstrap\n",
    "\n",
    "n = len(X)\n",
    "mu_hat = X.mean()\n",
    "sigma_hat = X.std()\n",
    "tau_hat = mu_hat + z_05 * sigma_hat\n",
    "\n",
    "B = 10000\n",
    "t_boot = np.empty(B)\n",
    "for i in range(B):\n",
    "    xx = norm.rvs(loc=mu_hat, scale=sigma_hat, size=n)\n",
    "    t_boot[i] = np.quantile(xx, 0.95)\n",
    "    \n",
    "se_tau_hat_bootstrap = t_boot.std()\n",
    "confidence_interval = (tau_hat - z_025 * se_tau_hat_bootstrap, tau_hat + z_025 * se_tau_hat_bootstrap)\n",
    "\n",
    "print(\"Estimated tau (parametric bootstrap, 95%% confidence interval): \\t (%.3f, %.3f)\" % confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.4**  Let $X_1, \\dots, X_n \\sim \\text{Uniform}(0, \\theta)$.  Show that the MLE is consistent.\n",
    "\n",
    "Hint: Let $Y = \\max \\{ X_1, \\dots, X_n \\}$.  For any c, $\\mathbb{P}(Y < c) = \\mathbb{P}(X_1 < c, X_2 < c, \\dots, X_n < c) = \\mathbb{P}(X_1 < c)\\mathbb{P}(X_2 < c)\\dots\\mathbb{P}(X_n < c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The probability density function is\n",
    "\n",
    "$$ f(x, \\theta) = \\mathbb{P}(Y < x) = \\prod_{i = 1}^n \\mathbb{P}(X_i < x) = f_{\\text{Uniform}(0, \\theta)}(x)^n $$\n",
    "\n",
    "The probability density function for the original distribution is\n",
    "\n",
    "$$ f_{\\text{Uniform}(0, \\theta)}(x) = \\begin{cases}\n",
    "\\theta^{-1} & \\text{if } 0 \\leq x \\leq \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$ f(x, \\theta) = \\begin{cases}\n",
    "\\theta^{-n} & \\text{if } 0 \\leq x \\leq \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The likelihood is maximized when $\\theta$ is as small as possible while keeping all samples within the first case, so $\\hat{\\theta}_n = \\max \\{X_1, \\dots, X_n \\}$.\n",
    "\n",
    "For a given $\\epsilon > 0$, we have\n",
    "\n",
    "$$\\mathbb{P}(\\hat{\\theta}_n < \\theta - \\epsilon) = \\prod_{i=1}^n \\mathbb{P}(X_i < \\theta - \\epsilon) = \\left(1 - \\frac{\\epsilon}{\\theta} \\right)^n$$\n",
    "\n",
    "which goes to 0 as $n \\rightarrow \\infty$, so $\\lim _{n \\rightarrow \\infty} \\hat{\\theta}_n = \\theta$, and thus the MLE is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.5**.  Let $X_1, \\dots, X_n \\sim \\text{Poisson}(\\lambda)$.  Find the method of moments estimator, the maximum likelihood estimator, and the Fisher information $I(\\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The first moment is:\n",
    "\n",
    "$$\\mathbb{E}(X) = \\lambda$$\n",
    "\n",
    "We have the sample moment:\n",
    "\n",
    "$$\\hat{\\alpha}_1 = \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
    "\n",
    "Equating those, the method of moments estimator for $\\hat{\\lambda}$ is:\n",
    "\n",
    "$$\\hat{\\lambda} = \\hat{\\alpha_1} = \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
    "\n",
    "The likelihood function is\n",
    "\n",
    "$$\\mathcal{L}_n(\\lambda) = \\prod_{i=1}^n f(X_i; \\lambda) = \\prod_{i=1}^n \\frac{\\lambda^{X_i}e^{-\\lambda}}{(X_i)!}$$\n",
    "\n",
    "so the log likelihood function is\n",
    "\n",
    "$$\\ell_n(\\lambda) = \\log \\mathcal{L}_n(\\lambda) = \\sum_{i=1}^n (\\log(\\lambda^{X_i}e^{-\\lambda}) - \\log X_i!)\n",
    "= \\sum_{i=1}^n (X_i \\log \\lambda - \\lambda - \\log X_i!)\n",
    "= -n \\lambda + (\\log \\lambda) \\sum_{i=1}^n X_i - \\sum_{i=1}^n \\log X_i!\n",
    "$$\n",
    "\n",
    "To find the MLE, we differentiate this equation with respect to 0 and equate it to 0:\n",
    "\n",
    "$$ \\frac{\\partial \\ell_n(\\lambda)}{\\partial \\lambda} = 0 \\\\\n",
    "-n + \\frac{\\sum_{i=1}^n X_i}{\\hat{\\lambda}} = 0 \\\\\n",
    "\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^n X_i\n",
    "$$\n",
    "\n",
    "The score function is:\n",
    "\n",
    "$$ s(X; \\lambda) = \\frac{\\partial \\log f(X; \\lambda)}{\\partial \\lambda} = \\frac{X}{\\lambda} - 1$$\n",
    "\n",
    "and the Fisher information is:\n",
    "\n",
    "$$ I_n(\\lambda) = \\sum_{i=1}^n \\mathbb{V}\\left( s(X_i; \\lambda) \\right) \n",
    "= \\sum_{i=1}^n \\mathbb{V} \\left( \\frac{X_i}{\\lambda} - 1 \\right)\n",
    "= \\frac{1}{\\lambda^2}  \\sum_{i=1}^n \\mathbb{V}(X_i) = \\frac{n}{\\lambda}$$\n",
    "\n",
    "In particular, $I(\\lambda) = I_1(\\lambda) = 1 / \\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.6**.  Let $X_1, \\dots, X_n \\sim N(\\theta, 1)$.  Define\n",
    "\n",
    "$$Y_i = \\begin{cases}\n",
    "1 & \\text{if } X_i > 0 \\\\\n",
    "0 & \\text{if } X_i \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Let $\\psi = \\mathbb{P}(Y_1 = 1)$.\n",
    "\n",
    "**(a)**  Find the maximum likelihood estimate $\\hat{\\psi}$ of $\\psi$.\n",
    "\n",
    "**(b)**  Find an approximate 95% confidence interval for $\\psi$.\n",
    "\n",
    "**(c)**  Define $\\overline{\\psi} = (1 / n) \\sum_i Y_i$.  Show that $\\overline{\\psi}$ is a consistent estimator of $\\psi$.\n",
    "\n",
    "**(d)**  Compute the asymptotic relative efficiency of $\\overline{\\psi}$ to $\\hat{\\psi}$.  Hint:  Use the delta method to get the standard error of the MLE.  Then compute the standard error (i.e. the standard deviation) of $\\overline{\\psi}$.\n",
    "\n",
    "**(e)**  Suppose that the data are not really normal.  Show that $\\psi$ is not consistent.  What, if anything, does $\\hat{\\psi}$ converge to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "Note that, from the definition, $Y_1, \\dots, Y_n \\sim \\text{Bernoulli}(\\Phi(\\theta))$, where $\\Phi$ is the CDF for the normal distribution.  Let $p = \\Phi(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have $\\psi = \\mathbb{P}(Y_1 = 1) = p$, so the MLE is $\\hat{\\psi} = \\hat{p} = \\Phi(\\hat{\\theta})\n",
    "= \\Phi(\\overline{X})$, where $\\overline{X} = n^{-1} \\sum_{i=1}^n X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Let $g(\\theta) = \\Phi(\\theta)$.  Then $g'(\\theta) = \\phi(\\theta)$, where $\\phi$ is the standard normal PDF.  By the delta method, $\\hat{\\text{se}}(\\hat{\\psi}) = |g'(\\hat{\\theta})| \\hat{\\text{se}}(\\hat{\\theta}) = \\phi(\\overline{X}) n^{-1/2}$.\n",
    "\n",
    "Then, an approximate 95% confidence interval is\n",
    "\n",
    "$$ C_n = \\left(\\Phi(\\overline{X}) \\left(1 - \\frac{z_{2.5\\%}}{\\sqrt{n}}\\right), \\; \n",
    "\\Phi(\\overline{X}) \\left(1 + \\frac{z_{2.5\\%}}{\\sqrt{n}}\\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  $\\overline{\\psi}$ has mean $p$, so consistency follows from the law of large numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** We have $\\mathbb{V}(Y_1) = \\psi (1 - \\psi)$, since $Y_1$ follows a Bernoulli distribution, so $\\mathbb{V}(\\overline{\\psi}) = \\mathbb{V}(Y_1) / n = \\psi (1 - \\psi) / n$.\n",
    "\n",
    "From (b), $\\mathbb{V}{\\hat{\\psi}} = \\phi(\\theta) / n$.\n",
    "\n",
    "Therefore, the asymptotic relative efficiency is\n",
    "\n",
    "$$\\frac{\\psi(1 - \\psi)}{\\phi(\\theta)} = \\frac{\\Phi(\\theta)(1 - \\Phi(\\theta))}{\\phi(\\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** By the law of large numbers, we still have that $\\overline{X}$ converges to $\\mathbb{E}(Y_1) = \\mathbb{P}(Y_1 = 1) \\cdot 1 + \\mathbb{P}(Y_1 = 0)\\cdot 0 = \\mathbb{P}(Y_1 = 1) = 1 - F_X(0) = \\mu_Y$.  Then $\\hat{\\psi} = \\Phi(\\overline{X})$ converges to $\\Phi(\\mu_Y)$.  But the true value of $\\psi$ is $\\mathbb{P}(Y_1 = 1) = 1 - F_X(0)$.\n",
    "\n",
    "But for an arbitrary distribution $1 - F_X(0) \\neq \\Phi(1 - F_X(0))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.7**. (Comparing two treatments).  $n_1$ people are given treatment 1 and $n_2$ people are given treatment 2.  Let $X_1$ be the number of people on treatment 1 who respond favorably to the treatment and let $X_2$ be the number of people on treatment 2 who respond favorably.  Assume that $X_1 \\sim \\text{Binomial}(n_1, p_1)$, $X_2 \\sim \\text{Binomial}(n_2, p_2)$.  Let $\\psi = p_1 - p_2$.\n",
    "\n",
    "**(a)** Find the MLE of $\\psi$.\n",
    "\n",
    "**(b)** Find the Fisher Information Matrix $I(p_1, p_2)$.\n",
    "\n",
    "**(c)** Use the multiparameter delta method to find the asymptotic standard error of $\\hat{\\psi}$.\n",
    "\n",
    "**(d)** Suppose that $n_1 = n_2 = 200$, $X_1 = 160$ and $X_2 = 148$.  Find $\\hat{\\psi}$.  Find an approximate 90% confidence interval for $\\psi$ using (i) the delta method and (ii) the parametric bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "**(a)** The MLE is equivariant, so\n",
    "\n",
    "$$\\hat{\\psi} = \\hat{p_1} - \\hat{p_2} = \\frac{X_1}{n_1} - \\frac{X_2}{n_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "The probability mass function is\n",
    "\n",
    "$$f((x_1, x_2); \\psi) = f_1(x_1; p_1) f_2(x_2; p_2) = \n",
    "\\binom{n_1}{x_1} p_1^{x_1} (1 - p_1)^{n_1 - x_1}\n",
    "\\binom{n_2}{x_2} p_2^{x_2} (1 - p_2)^{n_2 - x_2}\n",
    "$$\n",
    "\n",
    "The log likelihood is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell_n &= \\log f((x_1, x_2); \\psi) \\\\\n",
    "&= \\sum_{i=1}^2 \\left[\\log \\binom{n_i}{x_i} + x_i \\log p_i + (n_i - x_i) \\log (1 - p_i)\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "Calculating the partial derivatives and their expectations,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_{11} & = \\frac{\\partial^2 \\ell_n}{\\partial p_1^2}\n",
    "= \\frac{\\partial}{\\partial p_1} \\left( \\frac{x_1}{p_1} - \\frac{n_1 - x_1}{1 - p_1}\\right)\n",
    "= -\\frac{x_1}{p_1^2} - \\frac{n_1 - x_1}{(1 - p_1)^2} \\\\\n",
    "\\mathbb{E}[H_{11}] &= -\\frac{\\mathbb{E}[x_1]}{p_1^2} - \\frac{\\mathbb{E}[n - x_1]}{(1 - p_1)^2}\n",
    "= -\\frac{n_1  p_1}{p_1^2} - \\frac{n_1(1 - p_1)}{(1 - p_1)^2}\n",
    "= -\\frac{n_1}{p_1} - \\frac{n_1}{1 - p_1} = -\\frac{n_1}{p_1(1 - p_1)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "H_{22} &= -\\frac{x_2}{p_2^2} - \\frac{n_2 - x_2}{(1 - p_2)^2} \\\\\n",
    "\\mathbb{E}[H_{22}] &= -\\frac{n_2}{p_2(1 - p_2)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$ H_{12} = \\frac{\\partial^2 \\ell_n}{\\partial p_1 \\partial p_2} = 0$$\n",
    "$$ H_{21} = 0$$\n",
    "\n",
    "So the Fisher Information Matrix is:\n",
    "\n",
    "$$ I(p_1, p_2) = \\begin{bmatrix}\n",
    "\\frac{n_1}{p_1(1 - p_1)} & 0\\\\\n",
    "0 & \\frac{n_2}{p_2(1 - p_2)}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Using the multiparameter delta method, $g(\\psi) = p_1 - p_2$, so\n",
    "\n",
    "$$ \\nabla g = \\begin{bmatrix}\n",
    "\\partial g / \\partial p_1 \\\\ \\partial g / \\partial p_2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 \\\\ -1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The inverse of the Fisher Information Matrix is\n",
    "\n",
    "$$J(p_1, p_2) = I(p_1, p_2)^{-1} = \\begin{bmatrix}\n",
    "\\frac{p_1(1 - p_1)}{n_1} & 0 \\\\\n",
    "0 & \\frac{p_2(1 - p_2)}{n_2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then the asymptotic standard error of $\\hat{\\psi}$ is:\n",
    "\n",
    "$$\\hat{\\text{se}}(\\hat{\\psi}) = \\sqrt{(\\hat{\\nabla} g)^T \\hat{J}_n (\\hat{\\nabla} g)}\n",
    "= \\sqrt{\\frac{p_1(1 - p_1)}{n_1} + \\frac{p_2(1 - p_2)}{n_2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, binom\n",
    "\n",
    "n = 200\n",
    "X1 = 160\n",
    "X2 = 148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated psi: \t 0.060\n"
     ]
    }
   ],
   "source": [
    "p1_hat = X1 / n\n",
    "p2_hat = X2 / n\n",
    "psi_hat = p1_hat - p2_hat\n",
    "\n",
    "print(\"Estimated psi: \\t %.3f\" % psi_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% confidence interval (delta method): \t -0.009, 0.129\n"
     ]
    }
   ],
   "source": [
    "# Confidence using delta method\n",
    "\n",
    "z = norm.ppf(.95)\n",
    "\n",
    "se_delta = np.sqrt(p1_hat * (1 - p1_hat)/n + p2_hat * (1 - p2_hat) / n)\n",
    "confidence_delta = (psi_hat - z * se_delta, psi_hat + z * se_delta)\n",
    "\n",
    "print(\"90%% confidence interval (delta method): \\t %.3f, %.3f\" % confidence_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% confidence interval (parametric bootstrap): \t -0.010, 0.130\n"
     ]
    }
   ],
   "source": [
    "# Confidence using parametric bootstrap\n",
    "\n",
    "B = 1000\n",
    "xx1 = binom.rvs(n, p1_hat, size=B)\n",
    "xx2 = binom.rvs(n, p2_hat, size=B)\n",
    "t_boot = xx1 / n - xx2 / n\n",
    "\n",
    "se_bootstrap = t_boot.std()\n",
    "confidence_delta = (psi_hat - z * se_bootstrap, psi_hat + z * se_bootstrap)\n",
    "\n",
    "print(\"90%% confidence interval (parametric bootstrap): \\t %.3f, %.3f\" % confidence_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.13.8**.  Find the Fisher information matrix for Example 10.29:\n",
    "\n",
    "Let $X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  The log likelihood is:\n",
    "\n",
    "$$\\ell_n = \\sum_i \\log f(x; (\\mu, \\sigma))\n",
    "= n \\left[ \\log \\left( \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\right) + \\left( -\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma} \\right)^2\\right) \\right]$$\n",
    "\n",
    "From this,\n",
    "\n",
    "$$H_{11} = \\frac{\\partial^2 \\ell_n}{\\partial \\mu^2} = -\\frac{n}{\\sigma^2}$$\n",
    "$$H_{22} = \\frac{\\partial^2 \\ell_n}{\\partial \\sigma^2} = -\\frac{n}{\\sigma^2} - \\frac{n}{\\sigma^2} = -\\frac{2n}{\\sigma^2}$$\n",
    "$$H_{12} = H_{21} = \\frac{\\partial^2 \\ell_n}{\\partial \\mu \\partial \\sigma} = 0$$\n",
    "\n",
    "So the Fisher Information Matrix is\n",
    "\n",
    "$$I(\\mu, \\sigma) = -\\begin{bmatrix}\n",
    "\\mathbb{E}[H_{11}] & \\mathbb{E}[H_{12}] \\\\\n",
    "\\mathbb{E}[H_{21}] & \\mathbb{E}[H_{22}]\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{n}{\\sigma^2} & 0 \\\\\n",
    "0 & \\frac{2n}{\\sigma^2}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises 10.13.9 and 10.13.10**.  See final exercises from chapter 9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
